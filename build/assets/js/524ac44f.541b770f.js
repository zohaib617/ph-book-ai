"use strict";(globalThis.webpackChunkphysical_ai_humanoid_robotics=globalThis.webpackChunkphysical_ai_humanoid_robotics||[]).push([[276],{7184:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>l,contentTitle:()=>a,default:()=>m,frontMatter:()=>s,metadata:()=>t,toc:()=>c});const t=JSON.parse('{"id":"modules/module-4-vla/whisper-voice-action","title":"Voice-to-Action (Whisper)","description":"Introduction","source":"@site/docs/modules/module-4-vla/01-whisper-voice-action.mdx","sourceDirName":"modules/module-4-vla","slug":"/modules/module-4-vla/whisper-voice-action","permalink":"/docs/modules/module-4-vla/whisper-voice-action","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/modules/module-4-vla/01-whisper-voice-action.mdx","tags":[],"version":"current","sidebarPosition":1,"frontMatter":{"sidebar_label":"Voice-to-Action (Whisper)","sidebar_position":1},"sidebar":"tutorialSidebar","previous":{"title":"Module 4: Vision-Language-Action (VLA)","permalink":"/docs/modules/module-4-vla/"},"next":{"title":"Cognitive Planning (LLMs \u2192 ROS 2 actions)","permalink":"/docs/modules/module-4-vla/llm-cognitive-planning"}}');var o=i(4848),r=i(8453);const s={sidebar_label:"Voice-to-Action (Whisper)",sidebar_position:1},a="Voice-to-Action (Whisper)",l={},c=[{value:"Introduction",id:"introduction",level:2},{value:"Whisper Architecture and Capabilities",id:"whisper-architecture-and-capabilities",level:2},{value:"Whisper Model Variants",id:"whisper-model-variants",level:3},{value:"Whisper Integration with ROS 2",id:"whisper-integration-with-ros-2",level:2},{value:"Basic Whisper Node Implementation",id:"basic-whisper-node-implementation",level:3},{value:"Real-time Audio Processing for Robotics",id:"real-time-audio-processing-for-robotics",level:2},{value:"Audio Input Handling",id:"audio-input-handling",level:3},{value:"Voice Command Processing Pipeline",id:"voice-command-processing-pipeline",level:2},{value:"Natural Language Understanding for Robotics",id:"natural-language-understanding-for-robotics",level:3},{value:"Whisper Optimization for Robotics",id:"whisper-optimization-for-robotics",level:2},{value:"Edge Deployment Considerations",id:"edge-deployment-considerations",level:3},{value:"Voice Activity Detection (VAD) Integration",id:"voice-activity-detection-vad-integration",level:2},{value:"Combining Whisper with VAD",id:"combining-whisper-with-vad",level:3},{value:"Integration with Humanoid Robot Systems",id:"integration-with-humanoid-robot-systems",level:2},{value:"Complete Voice Command Pipeline",id:"complete-voice-command-pipeline",level:3},{value:"Performance Optimization and Best Practices",id:"performance-optimization-and-best-practices",level:2},{value:"Optimizing Whisper for Robotics Applications",id:"optimizing-whisper-for-robotics-applications",level:3},{value:"Troubleshooting Common Issues",id:"troubleshooting-common-issues",level:2},{value:"Common Whisper Integration Problems",id:"common-whisper-integration-problems",level:3},{value:"RAG Summary",id:"rag-summary",level:2},{value:"Knowledge Check",id:"knowledge-check",level:2}];function d(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,r.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(n.header,{children:(0,o.jsx)(n.h1,{id:"voice-to-action-whisper",children:"Voice-to-Action (Whisper)"})}),"\n",(0,o.jsx)(n.h2,{id:"introduction",children:"Introduction"}),"\n",(0,o.jsx)(n.p,{children:"Voice-to-action systems enable humanoid robots to understand and respond to human speech commands. This chapter explores how OpenAI's Whisper speech recognition model can be integrated into robotics systems to create natural human-robot interaction. Whisper provides robust speech-to-text capabilities that can be used to interpret voice commands and convert them into actionable tasks for humanoid robots."}),"\n",(0,o.jsx)(n.h2,{id:"whisper-architecture-and-capabilities",children:"Whisper Architecture and Capabilities"}),"\n",(0,o.jsx)(n.p,{children:"Whisper is a general-purpose speech recognition model that offers several key advantages for robotics applications:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Multilingual Support"}),": Can recognize speech in multiple languages"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Robustness"}),": Works well in various acoustic environments"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Open Source"}),": Available under MIT license for research and commercial use"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Multiple Sizes"}),": From tiny models for edge devices to large models for high accuracy"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Speaker Identification"}),": Can identify different speakers in multi-person environments"]}),"\n"]}),"\n",(0,o.jsx)(n.h3,{id:"whisper-model-variants",children:"Whisper Model Variants"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"# Example of different Whisper model variants for robotics\nimport whisper\n\n# Available models (sorted by size and capability)\nmodel_sizes = {\n    'tiny': {  # ~39M parameters, suitable for edge devices\n        'params': 39000000,\n        'relative_speed': 'fastest',\n        'accuracy': 'lowest',\n        'memory_usage': 'lowest',\n        'recommended_for': 'real-time applications, edge robotics'\n    },\n    'base': {  # ~74M parameters\n        'params': 74000000,\n        'relative_speed': 'fast',\n        'accuracy': 'low',\n        'memory_usage': 'low',\n        'recommended_for': 'mobile robots, general applications'\n    },\n    'small': {  # ~244M parameters\n        'params': 244000000,\n        'relative_speed': 'medium',\n        'accuracy': 'medium',\n        'memory_usage': 'medium',\n        'recommended_for': 'stationary robots, better accuracy needed'\n    },\n    'medium': {  # ~769M parameters\n        'params': 769000000,\n        'relative_speed': 'slow',\n        'accuracy': 'high',\n        'memory_usage': 'high',\n        'recommended_for': 'high-accuracy applications'\n    },\n    'large': {  # ~1550M parameters, best accuracy\n        'params': 1550000000,\n        'relative_speed': 'slowest',\n        'accuracy': 'highest',\n        'memory_usage': 'highest',\n        'recommended_for': 'research, high-precision tasks'\n    }\n}\n"})}),"\n",(0,o.jsx)(n.h2,{id:"whisper-integration-with-ros-2",children:"Whisper Integration with ROS 2"}),"\n",(0,o.jsx)(n.h3,{id:"basic-whisper-node-implementation",children:"Basic Whisper Node Implementation"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"# Example ROS 2 node for Whisper speech recognition\nimport rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nfrom sensor_msgs.msg import AudioData\nfrom builtin_interfaces.msg import Time\nimport whisper\nimport numpy as np\nimport io\nimport wave\nfrom pydub import AudioSegment\n\nclass WhisperNode(Node):\n    def __init__(self):\n        super().__init__('whisper_node')\n\n        # Initialize Whisper model\n        self.model = whisper.load_model(\"small\")  # Choose appropriate size\n\n        # Subscriptions and publishers\n        self.audio_subscription = self.create_subscription(\n            AudioData,\n            'audio_input',\n            self.audio_callback,\n            10\n        )\n\n        self.text_publisher = self.create_publisher(\n            String,\n            'recognized_text',\n            10\n        )\n\n        self.command_publisher = self.create_publisher(\n            String,\n            'voice_command',\n            10\n        )\n\n        # Parameters\n        self.declare_parameter('model_size', 'small')\n        self.declare_parameter('language', 'en')\n        self.declare_parameter('energy_threshold', 1000)\n\n        self.model_size = self.get_parameter('model_size').value\n        self.language = self.get_parameter('language').value\n        self.energy_threshold = self.get_parameter('energy_threshold').value\n\n    def audio_callback(self, msg):\n        \"\"\"Process incoming audio data\"\"\"\n        try:\n            # Convert audio data to appropriate format for Whisper\n            audio_np = np.frombuffer(msg.data, dtype=np.int16)\n\n            # Convert to float32 (Whisper expects float32)\n            audio_float = audio_np.astype(np.float32) / 32768.0\n\n            # Perform speech recognition\n            result = self.model.transcribe(audio_float, language=self.language)\n            recognized_text = result['text'].strip()\n\n            if recognized_text:  # Only publish if text was recognized\n                # Publish recognized text\n                text_msg = String()\n                text_msg.data = recognized_text\n                self.text_publisher.publish(text_msg)\n\n                # Process for command extraction\n                self.process_command(recognized_text)\n\n                self.get_logger().info(f'Recognized: {recognized_text}')\n\n        except Exception as e:\n            self.get_logger().error(f'Error in audio processing: {e}')\n\n    def process_command(self, text):\n        \"\"\"Extract and publish commands from recognized text\"\"\"\n        # Simple command extraction (in real implementation, use NLP)\n        commands = self.extract_commands(text)\n\n        for command in commands:\n            cmd_msg = String()\n            cmd_msg.data = command\n            self.command_publisher.publish(cmd_msg)\n            self.get_logger().info(f'Command extracted: {command}')\n\n    def extract_commands(self, text):\n        \"\"\"Extract actionable commands from text\"\"\"\n        # This is a simplified example\n        # In practice, use more sophisticated NLP techniques\n        text_lower = text.lower()\n\n        commands = []\n        if 'move' in text_lower or 'go' in text_lower or 'walk' in text_lower:\n            commands.append(f'move_command: {text}')\n        elif 'stop' in text_lower:\n            commands.append('stop_command')\n        elif 'turn' in text_lower or 'rotate' in text_lower:\n            commands.append(f'turn_command: {text}')\n        elif 'pick' in text_lower or 'grasp' in text_lower:\n            commands.append(f'grasp_command: {text}')\n        else:\n            commands.append(f'unknown_command: {text}')\n\n        return commands\n"})}),"\n",(0,o.jsx)(n.h2,{id:"real-time-audio-processing-for-robotics",children:"Real-time Audio Processing for Robotics"}),"\n",(0,o.jsx)(n.h3,{id:"audio-input-handling",children:"Audio Input Handling"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'# Example of real-time audio processing for Whisper\nimport rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nimport pyaudio\nimport numpy as np\nimport threading\nimport queue\nimport time\n\nclass RealtimeAudioNode(Node):\n    def __init__(self):\n        super().__init__(\'realtime_audio_node\')\n\n        # Audio parameters\n        self.rate = 16000  # Whisper works well at 16kHz\n        self.chunk = 1024\n        self.format = pyaudio.paInt16\n        self.channels = 1\n        self.record_seconds = 3  # Process 3-second chunks\n\n        # Initialize PyAudio\n        self.audio = pyaudio.PyAudio()\n\n        # Audio data queue for processing\n        self.audio_queue = queue.Queue()\n\n        # Publishers\n        self.text_publisher = self.create_publisher(String, \'recognized_text\', 10)\n\n        # Initialize Whisper model\n        self.model = whisper.load_model("base")\n\n        # Start audio capture thread\n        self.capture_thread = threading.Thread(target=self.audio_capture_thread)\n        self.capture_thread.daemon = True\n        self.capture_thread.start()\n\n        # Start processing timer\n        self.process_timer = self.create_timer(3.0, self.process_audio_chunk)\n\n    def audio_capture_thread(self):\n        """Capture audio in a separate thread"""\n        stream = self.audio.open(\n            format=self.format,\n            channels=self.channels,\n            rate=self.rate,\n            input=True,\n            frames_per_buffer=self.chunk\n        )\n\n        frames = []\n        while rclpy.ok():\n            data = stream.read(self.chunk)\n            frames.append(data)\n\n            # Keep only the last record_seconds of audio\n            if len(frames) * self.chunk / self.rate > self.record_seconds:\n                frames.pop(0)\n\n        stream.stop_stream()\n        stream.close()\n\n    def process_audio_chunk(self):\n        """Process accumulated audio chunk"""\n        # Get recent audio data\n        stream = self.audio.open(\n            format=self.format,\n            channels=self.channels,\n            rate=self.rate,\n            input=True,\n            frames_per_buffer=self.chunk\n        )\n\n        frames = []\n        for _ in range(int(self.rate / self.chunk * self.record_seconds)):\n            data = stream.read(self.chunk)\n            frames.append(data)\n\n        stream.stop_stream()\n        stream.close()\n\n        # Convert to numpy array\n        audio_data = b\'\'.join(frames)\n        audio_np = np.frombuffer(audio_data, dtype=np.int16)\n        audio_float = audio_np.astype(np.float32) / 32768.0\n\n        # Perform transcription\n        try:\n            result = self.model.transcribe(audio_float)\n            if result[\'text\'].strip():\n                text_msg = String()\n                text_msg.data = result[\'text\'].strip()\n                self.text_publisher.publish(text_msg)\n                self.get_logger().info(f\'Heard: {result["text"]}\')\n        except Exception as e:\n            self.get_logger().error(f\'Transcription error: {e}\')\n'})}),"\n",(0,o.jsx)(n.h2,{id:"voice-command-processing-pipeline",children:"Voice Command Processing Pipeline"}),"\n",(0,o.jsx)(n.h3,{id:"natural-language-understanding-for-robotics",children:"Natural Language Understanding for Robotics"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"# Example of NLU pipeline for voice commands\nimport rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nfrom geometry_msgs.msg import Twist\nfrom std_srvs.srv import Trigger\nimport spacy\nimport re\n\nclass VoiceCommandProcessorNode(Node):\n    def __init__(self):\n        super().__init__('voice_command_processor')\n\n        # Load spaCy model for NLP\n        try:\n            self.nlp = spacy.load(\"en_core_web_sm\")\n        except OSError:\n            self.get_logger().error(\"Please install en_core_web_sm: python -m spacy download en_core_web_sm\")\n            self.nlp = None\n\n        # Subscriptions\n        self.text_subscription = self.create_subscription(\n            String,\n            'recognized_text',\n            self.text_callback,\n            10\n        )\n\n        # Publishers for different robot capabilities\n        self.cmd_vel_publisher = self.create_publisher(Twist, 'cmd_vel', 10)\n        self.navigation_publisher = self.create_publisher(String, 'navigation_goal', 10)\n        self.arm_publisher = self.create_publisher(String, 'arm_command', 10)\n\n        # Service clients\n        self.speech_client = self.create_client(Trigger, 'speak_text')\n\n    def text_callback(self, msg):\n        \"\"\"Process recognized text for commands\"\"\"\n        text = msg.data.lower().strip()\n\n        if not text:\n            return\n\n        self.get_logger().info(f'Processing command: {text}')\n\n        # Parse the command\n        parsed_command = self.parse_command(text)\n\n        if parsed_command:\n            self.execute_command(parsed_command)\n\n    def parse_command(self, text):\n        \"\"\"Parse text command into structured command\"\"\"\n        if not self.nlp:\n            return self.simple_parse(text)\n\n        doc = self.nlp(text)\n\n        # Extract entities and dependencies\n        command_type = None\n        direction = None\n        distance = None\n        object_name = None\n\n        for token in doc:\n            if token.lemma_ in ['move', 'go', 'walk', 'navigate', 'drive']:\n                command_type = 'navigation'\n            elif token.lemma_ in ['turn', 'rotate', 'spin']:\n                command_type = 'rotation'\n            elif token.lemma_ in ['stop', 'halt', 'pause']:\n                command_type = 'stop'\n            elif token.lemma_ in ['pick', 'grasp', 'take', 'grab']:\n                command_type = 'manipulation'\n            elif token.lemma_ in ['speak', 'say', 'tell']:\n                command_type = 'speech'\n\n        # Extract direction\n        for token in doc:\n            if token.lemma_ in ['forward', 'ahead', 'front']:\n                direction = 'forward'\n            elif token.lemma_ in ['backward', 'back', 'reverse']:\n                direction = 'backward'\n            elif token.lemma_ in ['left', 'port']:\n                direction = 'left'\n            elif token.lemma_ in ['right', 'starboard']:\n                direction = 'right'\n\n        # Extract distance/duration\n        for ent in doc.ents:\n            if ent.label_ == \"CARDINAL\" or ent.label_ == \"MONEY\":\n                distance = ent.text\n\n        return {\n            'type': command_type,\n            'direction': direction,\n            'distance': distance,\n            'raw_text': text\n        }\n\n    def simple_parse(self, text):\n        \"\"\"Simple command parsing without NLP\"\"\"\n        command = {'type': None, 'direction': None, 'distance': None, 'raw_text': text}\n\n        if any(word in text for word in ['move', 'go', 'walk']):\n            command['type'] = 'navigation'\n        elif any(word in text for word in ['turn', 'rotate']):\n            command['type'] = 'rotation'\n        elif any(word in text for word in ['stop', 'halt']):\n            command['type'] = 'stop'\n        elif any(word in text for word in ['pick', 'grasp', 'take']):\n            command['type'] = 'manipulation'\n\n        if 'forward' in text or 'ahead' in text:\n            command['direction'] = 'forward'\n        elif 'backward' in text or 'back' in text:\n            command['direction'] = 'backward'\n        elif 'left' in text:\n            command['direction'] = 'left'\n        elif 'right' in text:\n            command['direction'] = 'right'\n\n        # Extract numbers for distance/duration\n        numbers = re.findall(r'\\d+', text)\n        if numbers:\n            command['distance'] = numbers[0]\n\n        return command\n\n    def execute_command(self, parsed_command):\n        \"\"\"Execute the parsed command\"\"\"\n        cmd_type = parsed_command['type']\n\n        if cmd_type == 'navigation':\n            self.execute_navigation_command(parsed_command)\n        elif cmd_type == 'rotation':\n            self.execute_rotation_command(parsed_command)\n        elif cmd_type == 'stop':\n            self.execute_stop_command()\n        elif cmd_type == 'manipulation':\n            self.execute_manipulation_command(parsed_command)\n        elif cmd_type == 'speech':\n            self.execute_speech_command(parsed_command)\n\n    def execute_navigation_command(self, command):\n        \"\"\"Execute navigation command\"\"\"\n        twist = Twist()\n\n        if command['direction'] == 'forward':\n            twist.linear.x = 0.5  # m/s\n        elif command['direction'] == 'backward':\n            twist.linear.x = -0.5\n        else:\n            twist.linear.x = 0.5  # default forward\n\n        # Set duration based on distance if specified\n        duration = float(command['distance']) if command['distance'] else 2.0\n\n        self.cmd_vel_publisher.publish(twist)\n        self.get_logger().info(f'Moving {command[\"direction\"] or \"forward\"} for {duration}s')\n\n    def execute_rotation_command(self, command):\n        \"\"\"Execute rotation command\"\"\"\n        twist = Twist()\n\n        if command['direction'] == 'left':\n            twist.angular.z = 0.5  # rad/s\n        elif command['direction'] == 'right':\n            twist.angular.z = -0.5\n        else:\n            twist.angular.z = 0.5  # default turn right\n\n        duration = float(command['distance']) if command['distance'] else 1.0\n\n        self.cmd_vel_publisher.publish(twist)\n        self.get_logger().info(f'Turning {command[\"direction\"] or \"right\"} for {duration}s')\n\n    def execute_stop_command(self):\n        \"\"\"Execute stop command\"\"\"\n        twist = Twist()  # Zero velocities\n        self.cmd_vel_publisher.publish(twist)\n        self.get_logger().info('Stopping robot')\n\n    def execute_manipulation_command(self, command):\n        \"\"\"Execute manipulation command\"\"\"\n        arm_cmd = String()\n        arm_cmd.data = command['raw_text']\n        self.arm_publisher.publish(arm_cmd)\n        self.get_logger().info(f'Arm command: {command[\"raw_text\"]}')\n\n    def execute_speech_command(self, command):\n        \"\"\"Execute speech command\"\"\"\n        if self.speech_client.wait_for_service(timeout_sec=1.0):\n            # This would call a text-to-speech service\n            pass\n        else:\n            self.get_logger().warn('Speech service not available')\n"})}),"\n",(0,o.jsx)(n.h2,{id:"whisper-optimization-for-robotics",children:"Whisper Optimization for Robotics"}),"\n",(0,o.jsx)(n.h3,{id:"edge-deployment-considerations",children:"Edge Deployment Considerations"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"# Example of optimized Whisper for edge robotics\nimport rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nimport torch\nimport whisper\nimport numpy as np\nfrom collections import deque\nimport time\n\nclass OptimizedWhisperNode(Node):\n    def __init__(self):\n        super().__init__('optimized_whisper_node')\n\n        # Check for GPU availability\n        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n        self.get_logger().info(f'Using device: {self.device}')\n\n        # Initialize model with optimization\n        self.model = whisper.load_model(\"base\").to(self.device)\n        self.model.eval()  # Set to evaluation mode\n\n        # Audio processing parameters\n        self.sample_rate = 16000  # Whisper standard\n        self.audio_buffer_size = 16000 * 3  # 3 seconds of audio\n        self.audio_buffer = deque(maxlen=self.audio_buffer_size)\n\n        # Subscriptions and publishers\n        self.audio_subscription = self.create_subscription(\n            String,  # In practice, this would be AudioData\n            'audio_buffer',\n            self.optimized_audio_callback,\n            10\n        )\n\n        self.text_publisher = self.create_publisher(String, 'recognized_text', 10)\n\n        # Performance monitoring\n        self.processing_times = deque(maxlen=10)\n\n        # Timer for periodic processing\n        self.process_timer = self.create_timer(2.0, self.process_audio_buffer)\n\n    def optimized_audio_callback(self, msg):\n        \"\"\"Optimized audio buffer management\"\"\"\n        # Convert audio data to float32\n        audio_data = np.frombuffer(msg.data.encode('latin1'), dtype=np.int16)\n        audio_float = audio_data.astype(np.float32) / 32768.0\n\n        # Add to circular buffer\n        for sample in audio_float:\n            self.audio_buffer.append(sample)\n\n    def process_audio_buffer(self):\n        \"\"\"Process audio buffer with Whisper\"\"\"\n        if len(self.audio_buffer) < self.audio_buffer_size // 2:\n            return  # Not enough audio data yet\n\n        # Convert buffer to numpy array\n        audio_array = np.array(list(self.audio_buffer))\n\n        # Measure processing time\n        start_time = time.time()\n\n        try:\n            # Process with Whisper\n            result = self.model.transcribe(audio_array, language='en')\n\n            processing_time = time.time() - start_time\n            self.processing_times.append(processing_time)\n\n            # Log performance\n            avg_time = sum(self.processing_times) / len(self.processing_times)\n            self.get_logger().info(f'Processing time: {processing_time:.2f}s (avg: {avg_time:.2f}s)')\n\n            if result['text'].strip():\n                text_msg = String()\n                text_msg.data = result['text'].strip()\n                self.text_publisher.publish(text_msg)\n\n        except Exception as e:\n            self.get_logger().error(f'Whisper processing error: {e}')\n"})}),"\n",(0,o.jsx)(n.h2,{id:"voice-activity-detection-vad-integration",children:"Voice Activity Detection (VAD) Integration"}),"\n",(0,o.jsx)(n.h3,{id:"combining-whisper-with-vad",children:"Combining Whisper with VAD"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'# Example of combining Whisper with Voice Activity Detection\nimport rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nimport numpy as np\nfrom scipy import signal\nimport threading\n\nclass VADWhisperNode(Node):\n    def __init__(self):\n        super().__init__(\'vad_whisper_node\')\n\n        # VAD parameters\n        self.energy_threshold = 1000\n        self.silence_duration = 0.5  # seconds\n        self.min_speech_duration = 0.5  # seconds\n\n        # Audio processing\n        self.audio_buffer = []\n        self.is_speaking = False\n        self.silence_start_time = None\n        self.speech_start_time = None\n\n        # Initialize Whisper\n        self.model = whisper.load_model("base")\n\n        # Publishers\n        self.text_publisher = self.create_publisher(String, \'recognized_text\', 10)\n\n        # Timer for audio processing\n        self.process_timer = self.create_timer(0.1, self.process_audio)\n\n    def calculate_energy(self, audio_chunk):\n        """Calculate energy of audio chunk for VAD"""\n        return np.mean(np.square(audio_chunk))\n\n    def detect_voice_activity(self, audio_chunk):\n        """Detect voice activity in audio chunk"""\n        energy = self.calculate_energy(audio_chunk)\n        return energy > self.energy_threshold\n\n    def process_audio(self):\n        """Process incoming audio for VAD and Whisper"""\n        # In a real implementation, this would get audio from a source\n        # For this example, we\'ll simulate the process\n\n        # This is where you would get audio data\n        # audio_chunk = self.get_audio_chunk()\n\n        # For simulation, we\'ll create a mock audio chunk\n        audio_chunk = np.random.random(160) * 0.1  # 10ms at 16kHz\n\n        voice_active = self.detect_voice_activity(audio_chunk)\n\n        if voice_active and not self.is_speaking:\n            # Speech started\n            self.is_speaking = True\n            self.speech_start_time = time.time()\n            self.audio_buffer = []  # Start new buffer\n            self.get_logger().info(\'Speech detected, starting recording\')\n\n        if self.is_speaking:\n            # Add to buffer\n            self.audio_buffer.extend(audio_chunk)\n\n            if not voice_active:\n                # Silence detected\n                if self.silence_start_time is None:\n                    self.silence_start_time = time.time()\n            else:\n                # Voice detected, reset silence timer\n                self.silence_start_time = None\n\n        # Check if we should process the audio\n        if (self.is_speaking and\n            self.silence_start_time and\n            time.time() - self.silence_start_time > self.silence_duration):\n\n            # Enough silence, process the speech\n            self.process_speech_segment()\n\n        # Check if buffer is getting too large\n        max_buffer_duration = 5.0  # Maximum 5 seconds\n        if (self.is_speaking and\n            len(self.audio_buffer) / 16000 > max_buffer_duration):\n            self.process_speech_segment()\n\n    def process_speech_segment(self):\n        """Process collected speech segment with Whisper"""\n        if len(self.audio_buffer) < 1600:  # At least 0.1 seconds\n            self.reset_speaking_state()\n            return\n\n        # Convert buffer to numpy array\n        audio_array = np.array(self.audio_buffer, dtype=np.float32)\n\n        try:\n            # Transcribe with Whisper\n            result = self.model.transcribe(audio_array)\n\n            if result[\'text\'].strip():\n                text_msg = String()\n                text_msg.data = result[\'text\'].strip()\n                self.text_publisher.publish(text_msg)\n                self.get_logger().info(f\'Recognized: {result["text"]}\')\n\n        except Exception as e:\n            self.get_logger().error(f\'Whisper error: {e}\')\n\n        self.reset_speaking_state()\n\n    def reset_speaking_state(self):\n        """Reset voice activity detection state"""\n        self.is_speaking = False\n        self.silence_start_time = None\n        self.speech_start_time = None\n        self.audio_buffer = []\n        self.get_logger().info(\'Reset speech detection state\')\n'})}),"\n",(0,o.jsx)(n.h2,{id:"integration-with-humanoid-robot-systems",children:"Integration with Humanoid Robot Systems"}),"\n",(0,o.jsx)(n.h3,{id:"complete-voice-command-pipeline",children:"Complete Voice Command Pipeline"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'# Example of complete voice-to-action pipeline for humanoid robots\nimport rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nfrom geometry_msgs.msg import Twist, Pose\nfrom sensor_msgs.msg import JointState\nfrom builtin_interfaces.msg import Duration\nfrom action_msgs.msg import GoalStatus\nfrom rclpy.action import ActionClient\nfrom control_msgs.action import FollowJointTrajectory\nfrom trajectory_msgs.msg import JointTrajectory, JointTrajectoryPoint\n\nclass HumanoidVoiceControllerNode(Node):\n    def __init__(self):\n        super().__init__(\'humanoid_voice_controller\')\n\n        # Initialize Whisper model\n        self.whisper_model = whisper.load_model("small")\n\n        # Command mapping\n        self.command_mapping = {\n            \'walk forward\': self.walk_forward,\n            \'walk backward\': self.walk_backward,\n            \'turn left\': self.turn_left,\n            \'turn right\': self.turn_right,\n            \'raise arms\': self.raise_arms,\n            \'lower arms\': self.lower_arms,\n            \'wave\': self.wave,\n            \'dance\': self.dance,\n            \'stop\': self.stop_robot\n        }\n\n        # Publishers for different robot systems\n        self.cmd_vel_publisher = self.create_publisher(Twist, \'cmd_vel\', 10)\n        self.joint_trajectory_client = ActionClient(\n            self,\n            FollowJointTrajectory,\n            \'joint_trajectory_controller/follow_joint_trajectory\'\n        )\n\n        # Voice command subscription\n        self.voice_subscription = self.create_subscription(\n            String,\n            \'voice_command\',\n            self.voice_command_callback,\n            10\n        )\n\n        self.get_logger().info(\'Humanoid Voice Controller initialized\')\n\n    def voice_command_callback(self, msg):\n        """Process voice command"""\n        command_text = msg.data.lower()\n        self.get_logger().info(f\'Received voice command: {command_text}\')\n\n        # Find best matching command\n        best_match = self.find_best_command_match(command_text)\n\n        if best_match:\n            self.get_logger().info(f\'Executing command: {best_match}\')\n            self.command_mapping[best_match]()\n        else:\n            self.get_logger().warn(f\'Unknown command: {command_text}\')\n            self.execute_fallback_behavior(command_text)\n\n    def find_best_command_match(self, text):\n        """Find best matching command for the text"""\n        # Simple keyword matching (in practice, use more sophisticated NLP)\n        for command in self.command_mapping.keys():\n            if all(word in text for word in command.split()):\n                return command\n\n        # Partial matching\n        for command in self.command_mapping.keys():\n            if any(word in text for word in command.split()):\n                return command\n\n        return None\n\n    def walk_forward(self):\n        """Make humanoid walk forward"""\n        # This would involve complex bipedal locomotion\n        # For this example, we\'ll send a simple velocity command\n        twist = Twist()\n        twist.linear.x = 0.3  # Forward velocity\n        self.cmd_vel_publisher.publish(twist)\n\n    def walk_backward(self):\n        """Make humanoid walk backward"""\n        twist = Twist()\n        twist.linear.x = -0.3  # Backward velocity\n        self.cmd_vel_publisher.publish(twist)\n\n    def turn_left(self):\n        """Make humanoid turn left"""\n        twist = Twist()\n        twist.angular.z = 0.5  # Left turn\n        self.cmd_vel_publisher.publish(twist)\n\n    def turn_right(self):\n        """Make humanoid turn right"""\n        twist = Twist()\n        twist.angular.z = -0.5  # Right turn\n        self.cmd_vel_publisher.publish(twist)\n\n    def raise_arms(self):\n        """Raise humanoid arms"""\n        # Create joint trajectory for raising arms\n        trajectory = JointTrajectory()\n        trajectory.joint_names = [\'left_shoulder_joint\', \'right_shoulder_joint\']\n\n        point = JointTrajectoryPoint()\n        point.positions = [1.57, 1.57]  # Raise both arms\n        point.time_from_start = Duration(sec=2)  # 2 seconds to complete\n\n        trajectory.points = [point]\n\n        self.send_joint_trajectory(trajectory)\n\n    def lower_arms(self):\n        """Lower humanoid arms"""\n        trajectory = JointTrajectory()\n        trajectory.joint_names = [\'left_shoulder_joint\', \'right_shoulder_joint\']\n\n        point = JointTrajectoryPoint()\n        point.positions = [0.0, 0.0]  # Lower both arms\n        point.time_from_start = Duration(sec=2)\n\n        trajectory.points = [point]\n\n        self.send_joint_trajectory(trajectory)\n\n    def wave(self):\n        """Make humanoid wave"""\n        trajectory = JointTrajectory()\n        trajectory.joint_names = [\'right_shoulder_joint\', \'right_elbow_joint\']\n\n        # Wave motion: raise and rotate\n        point1 = JointTrajectoryPoint()\n        point1.positions = [1.57, 0.0]\n        point1.time_from_start = Duration(sec=1)\n\n        point2 = JointTrajectoryPoint()\n        point2.positions = [1.57, 1.57]\n        point2.time_from_start = Duration(sec=2)\n\n        point3 = JointTrajectoryPoint()\n        point3.positions = [1.57, 0.0]\n        point3.time_from_start = Duration(sec=3)\n\n        trajectory.points = [point1, point2, point3]\n\n        self.send_joint_trajectory(trajectory)\n\n    def dance(self):\n        """Make humanoid dance"""\n        # This would involve a complex sequence of movements\n        # For this example, we\'ll do a simple dance sequence\n        self.wave()\n        self.get_logger().info(\'Dancing! (simplified version)\')\n\n    def stop_robot(self):\n        """Stop all robot movement"""\n        twist = Twist()  # Zero velocities\n        self.cmd_vel_publisher.publish(twist)\n\n    def send_joint_trajectory(self, trajectory):\n        """Send joint trajectory to robot"""\n        if self.joint_trajectory_client.wait_for_server(timeout_sec=1.0):\n            goal_msg = FollowJointTrajectory.Goal()\n            goal_msg.trajectory = trajectory\n\n            self.joint_trajectory_client.send_goal_async(goal_msg)\n        else:\n            self.get_logger().error(\'Joint trajectory server not available\')\n\n    def execute_fallback_behavior(self, command_text):\n        """Execute fallback behavior for unknown commands"""\n        # In a real system, you might query an LLM for interpretation\n        self.get_logger().info(f\'Trying to interpret: {command_text}\')\n\n        # For now, just stop\n        self.stop_robot()\n'})}),"\n",(0,o.jsx)(n.h2,{id:"performance-optimization-and-best-practices",children:"Performance Optimization and Best Practices"}),"\n",(0,o.jsx)(n.h3,{id:"optimizing-whisper-for-robotics-applications",children:"Optimizing Whisper for Robotics Applications"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'# Example of performance optimization techniques\nclass WhisperOptimizer:\n    @staticmethod\n    def optimize_for_latency(model_size="base"):\n        """Optimize Whisper for low latency"""\n        # Use smaller model for faster processing\n        # Implement streaming for real-time processing\n        # Use quantization for faster inference\n        return whisper.load_model(model_size)\n\n    @staticmethod\n    def optimize_for_accuracy(model_size="large"):\n        """Optimize Whisper for high accuracy"""\n        # Use larger model for better accuracy\n        # Use beam search for better results\n        # Apply language-specific fine-tuning\n        return whisper.load_model(model_size)\n\n    @staticmethod\n    def streaming_transcription(audio_stream, model, chunk_size=16000):\n        """Perform streaming transcription"""\n        # This would implement real streaming\n        # For now, return a placeholder\n        pass\n\n# Best practices summary\n"""\nWhisper Best Practices for Robotics:\n\n1. Model Selection:\n   - Use \'base\' or \'small\' for real-time applications\n   - Use \'large\' for high-accuracy scenarios\n   - Consider hardware constraints\n\n2. Audio Preprocessing:\n   - Ensure 16kHz sample rate\n   - Apply noise reduction\n   - Use proper microphone placement\n\n3. Performance Optimization:\n   - Use GPU acceleration when available\n   - Implement audio buffering\n   - Consider quantized models\n\n4. Robustness:\n   - Handle different accents and languages\n   - Implement voice activity detection\n   - Add confidence scoring\n"""\n'})}),"\n",(0,o.jsx)(n.h2,{id:"troubleshooting-common-issues",children:"Troubleshooting Common Issues"}),"\n",(0,o.jsx)(n.h3,{id:"common-whisper-integration-problems",children:"Common Whisper Integration Problems"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Audio Format Issues"}),": Ensure audio is properly formatted for Whisper (16kHz, float32)"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Performance Problems"}),": Use appropriate model size for hardware constraints"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Accuracy Issues"}),": Consider fine-tuning for domain-specific vocabulary"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Latency Problems"}),": Implement streaming or buffering strategies"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Environmental Noise"}),": Use noise reduction and VAD techniques"]}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"rag-summary",children:"RAG Summary"}),"\n",(0,o.jsx)(n.p,{children:"Whisper provides robust speech recognition capabilities for humanoid robotics applications. The integration involves real-time audio processing, voice activity detection, and natural language understanding to convert voice commands into actionable robot behaviors. Proper optimization and model selection are crucial for real-time performance on robotic platforms."}),"\n",(0,o.jsx)(n.h2,{id:"knowledge-check",children:"Knowledge Check"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsx)(n.li,{children:"What are the different Whisper model variants and their trade-offs?"}),"\n",(0,o.jsx)(n.li,{children:"How do you integrate Whisper with ROS 2 for real-time processing?"}),"\n",(0,o.jsx)(n.li,{children:"What is the role of Voice Activity Detection in robotics applications?"}),"\n",(0,o.jsx)(n.li,{children:"How do you map voice commands to robot actions?"}),"\n",(0,o.jsx)(n.li,{children:"What are the key optimization strategies for Whisper in robotics?"}),"\n"]})]})}function m(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(d,{...e})}):d(e)}},8453:(e,n,i)=>{i.d(n,{R:()=>s});var t=i(6540);const o={},r=t.createContext(o);function s(e){const n=t.useContext(r);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}}}]);