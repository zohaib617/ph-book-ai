"use strict";(globalThis.webpackChunkphysical_ai_humanoid_robotics=globalThis.webpackChunkphysical_ai_humanoid_robotics||[]).push([[359],{8393:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>r,default:()=>u,frontMatter:()=>i,metadata:()=>s,toc:()=>m});const s=JSON.parse('{"id":"modules/module-4-vla/autonomous-humanoid","title":"Capstone: Autonomous Humanoid Pipeline","description":"Introduction","source":"@site/docs/modules/module-4-vla/03-autonomous-humanoid.mdx","sourceDirName":"modules/module-4-vla","slug":"/modules/module-4-vla/autonomous-humanoid","permalink":"/docs/modules/module-4-vla/autonomous-humanoid","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/modules/module-4-vla/03-autonomous-humanoid.mdx","tags":[],"version":"current","sidebarPosition":3,"frontMatter":{"sidebar_label":"Capstone: Autonomous Humanoid Pipeline","sidebar_position":3},"sidebar":"tutorialSidebar","previous":{"title":"Cognitive Planning (LLMs \u2192 ROS 2 actions)","permalink":"/docs/modules/module-4-vla/llm-cognitive-planning"}}');var o=t(4848),a=t(8453);const i={sidebar_label:"Capstone: Autonomous Humanoid Pipeline",sidebar_position:3},r="Capstone: Autonomous Humanoid Pipeline",l={},m=[{value:"Introduction",id:"introduction",level:2},{value:"Complete System Architecture",id:"complete-system-architecture",level:2},{value:"High-Level System Overview",id:"high-level-system-overview",level:3},{value:"Complete System Launch",id:"complete-system-launch",level:2},{value:"Main Launch File",id:"main-launch-file",level:3},{value:"System Orchestrator",id:"system-orchestrator",level:2},{value:"Main Control Node",id:"main-control-node",level:3},{value:"Integration with Simulation Environments",id:"integration-with-simulation-environments",level:2},{value:"Isaac Sim Integration",id:"isaac-sim-integration",level:3},{value:"Real-World Deployment Considerations",id:"real-world-deployment-considerations",level:2},{value:"Hardware Abstraction Layer",id:"hardware-abstraction-layer",level:3},{value:"Performance Monitoring and Optimization",id:"performance-monitoring-and-optimization",level:2},{value:"System Performance Node",id:"system-performance-node",level:3},{value:"Testing and Validation Framework",id:"testing-and-validation-framework",level:2},{value:"System Integration Tests",id:"system-integration-tests",level:3},{value:"Deployment and Maintenance",id:"deployment-and-maintenance",level:2},{value:"System Deployment Scripts",id:"system-deployment-scripts",level:3},{value:"Troubleshooting and Debugging",id:"troubleshooting-and-debugging",level:2},{value:"Common Issues and Solutions",id:"common-issues-and-solutions",level:3},{value:"RAG Summary",id:"rag-summary",level:2},{value:"Knowledge Check",id:"knowledge-check",level:2},{value:"Next Steps",id:"next-steps",level:2}];function c(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,a.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(n.header,{children:(0,o.jsx)(n.h1,{id:"capstone-autonomous-humanoid-pipeline",children:"Capstone: Autonomous Humanoid Pipeline"})}),"\n",(0,o.jsx)(n.h2,{id:"introduction",children:"Introduction"}),"\n",(0,o.jsx)(n.p,{children:"This capstone chapter integrates all the concepts from the Physical AI & Humanoid Robotics book into a complete autonomous humanoid pipeline. We'll combine ROS 2 fundamentals, simulation environments, perception systems, and Vision-Language-Action capabilities into a unified system that can understand voice commands, navigate environments, and perform complex tasks autonomously."}),"\n",(0,o.jsx)(n.h2,{id:"complete-system-architecture",children:"Complete System Architecture"}),"\n",(0,o.jsx)(n.h3,{id:"high-level-system-overview",children:"High-Level System Overview"}),"\n",(0,o.jsx)(n.p,{children:"The autonomous humanoid pipeline consists of interconnected modules:"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Perception Layer"}),": Sensor processing and environment understanding"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Cognition Layer"}),": LLM-based reasoning and planning"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Action Layer"}),": Low-level control and execution"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Communication Layer"}),": Voice and multimodal interaction"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Integration Layer"}),": ROS 2 coordination and messaging"]}),"\n"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-yaml",children:'# Complete autonomous humanoid system configuration\n# Main launch file components\nautonomous_humanoid_system:\n  ros__parameters:\n    # System-wide parameters\n    system_name: "autonomous_humanoid"\n    operational_mode: "autonomous"  # autonomous, teleop, semi_autonomous\n    safety_level: "high"  # low, medium, high\n    max_operation_time: 3600  # seconds\n\nhumanoid_perception:\n  ros__parameters:\n    # Camera settings\n    camera_topic: "/camera/rgb/image_raw"\n    camera_info_topic: "/camera/rgb/camera_info"\n    image_processing_rate: 10.0  # Hz\n\n    # LiDAR settings\n    laser_topic: "/scan"\n    obstacle_detection_range: 5.0  # meters\n\n    # IMU settings\n    imu_topic: "/imu/data"\n    balance_threshold: 0.1  # radians\n\nhumanoid_cognition:\n  ros__parameters:\n    # LLM settings\n    llm_model: "gpt-3.5-turbo"\n    llm_api_key: "your_api_key_here"\n    max_tokens: 1000\n    temperature: 0.3\n\n    # Planning parameters\n    plan_timeout: 30.0  # seconds\n    max_plan_steps: 50\n    replanning_frequency: 1.0  # Hz\n\nhumanoid_control:\n  ros__parameters:\n    # Navigation parameters\n    navigation_stack: "nav2"\n    global_planner: "nav2_navfn_planner/NavfnPlanner"\n    local_planner: "dwb_core/DWBLocalPlanner"\n\n    # Manipulation parameters\n    arm_controller: "arm_controller"\n    gripper_controller: "gripper_controller"\n\n    # Balance control\n    balance_controller: "balance_controller"\n    zmp_margin: 0.05  # meters\n\nhumanoid_communication:\n  ros__parameters:\n    # Audio input\n    audio_input_topic: "/audio_input"\n    speech_recognition_model: "whisper-base"\n\n    # Text-to-speech\n    tts_service: "text_to_speech"\n    speech_volume: 0.8\n'})}),"\n",(0,o.jsx)(n.h2,{id:"complete-system-launch",children:"Complete System Launch"}),"\n",(0,o.jsx)(n.h3,{id:"main-launch-file",children:"Main Launch File"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"# Complete autonomous humanoid launch file\nimport os\nfrom launch import LaunchDescription\nfrom launch.actions import DeclareLaunchArgument, IncludeLaunchDescription, RegisterEventHandler\nfrom launch.conditions import IfCondition\nfrom launch.launch_description_sources import PythonLaunchDescriptionSource\nfrom launch.substitutions import LaunchConfiguration, PathJoinSubstitution\nfrom launch_ros.actions import Node, ComposableNodeContainer\nfrom launch_ros.descriptions import ComposableNode\nfrom launch_ros.substitutions import FindPackageShare\nfrom ament_index_python.packages import get_package_share_directory\n\ndef generate_launch_description():\n    # Launch configuration variables\n    use_sim_time = LaunchConfiguration('use_sim_time')\n    autostart = LaunchConfiguration('autostart')\n    log_level = LaunchConfiguration('log_level')\n\n    # Declare launch arguments\n    declare_use_sim_time = DeclareLaunchArgument(\n        'use_sim_time',\n        default_value='false',\n        description='Use simulation (Gazebo) clock if true'\n    )\n\n    declare_autostart = DeclareLaunchArgument(\n        'autostart',\n        default_value='true',\n        description='Automatically start the controllers'\n    )\n\n    declare_log_level = DeclareLaunchArgument(\n        'log_level',\n        default_value='info',\n        description='Log level for the nodes'\n    )\n\n    # Launch navigation system\n    navigation_launch = IncludeLaunchDescription(\n        PythonLaunchDescriptionSource([\n            PathJoinSubstitution([\n                FindPackageShare('nav2_bringup'),\n                'launch',\n                'navigation_launch.py'\n            ])\n        ]),\n        launch_arguments={\n            'use_sim_time': use_sim_time,\n            'autostart': autostart\n        }.items()\n    )\n\n    # Launch perception system\n    perception_container = ComposableNodeContainer(\n        name='perception_container',\n        namespace='',\n        package='rclcpp_components',\n        executable='component_container',\n        composable_node_descriptions=[\n            ComposableNode(\n                package='isaac_ros_apriltag',\n                plugin='nvidia::isaac_ros::apriltag::AprilTagNode',\n                name='apriltag',\n                parameters=[{\n                    'size': 0.16,\n                    'max_tags': 64,\n                    'tile_size': 1\n                }]\n            ),\n            ComposableNode(\n                package='isaac_ros_stereo_image_proc',\n                plugin='nvidia::isaac_ros::stereo_image_proc::DisparityNode',\n                name='disparity',\n                parameters=[{\n                    'approximate_sync': True\n                }]\n            )\n        ],\n        output='screen'\n    )\n\n    # Launch cognitive planning node\n    cognitive_planning_node = Node(\n        package='autonomous_humanoid',\n        executable='cognitive_planning_node',\n        name='cognitive_planner',\n        parameters=[\n            PathJoinSubstitution([\n                FindPackageShare('autonomous_humanoid'),\n                'config',\n                'cognitive_planning.yaml'\n            ]),\n            {'use_sim_time': use_sim_time}\n        ],\n        output='screen'\n    )\n\n    # Launch voice interaction node\n    voice_interaction_node = Node(\n        package='autonomous_humanoid',\n        executable='voice_interaction_node',\n        name='voice_interaction',\n        parameters=[\n            PathJoinSubstitution([\n                FindPackageShare('autonomous_humanoid'),\n                'config',\n                'voice_interaction.yaml'\n            ]),\n            {'use_sim_time': use_sim_time}\n        ],\n        output='screen'\n    )\n\n    # Launch balance controller\n    balance_controller_node = Node(\n        package='autonomous_humanoid',\n        executable='balance_controller_node',\n        name='balance_controller',\n        parameters=[\n            PathJoinSubstitution([\n                FindPackageShare('autonomous_humanoid'),\n                'config',\n                'balance_control.yaml'\n            ]),\n            {'use_sim_time': use_sim_time}\n        ],\n        output='screen'\n    )\n\n    # Launch main orchestrator\n    orchestrator_node = Node(\n        package='autonomous_humanoid',\n        executable='system_orchestrator_node',\n        name='system_orchestrator',\n        parameters=[\n            PathJoinSubstitution([\n                FindPackageShare('autonomous_humanoid'),\n                'config',\n                'system_config.yaml'\n            ]),\n            {'use_sim_time': use_sim_time}\n        ],\n        output='screen'\n    )\n\n    # Create launch description\n    ld = LaunchDescription()\n\n    # Add launch arguments\n    ld.add_action(declare_use_sim_time)\n    ld.add_action(declare_autostart)\n    ld.add_action(declare_log_level)\n\n    # Add nodes\n    ld.add_action(navigation_launch)\n    ld.add_action(perception_container)\n    ld.add_action(cognitive_planning_node)\n    ld.add_action(voice_interaction_node)\n    ld.add_action(balance_controller_node)\n    ld.add_action(orchestrator_node)\n\n    return ld\n"})}),"\n",(0,o.jsx)(n.h2,{id:"system-orchestrator",children:"System Orchestrator"}),"\n",(0,o.jsx)(n.h3,{id:"main-control-node",children:"Main Control Node"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'# System orchestrator - the "brain" of the autonomous humanoid\nimport rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String, Bool, Int8\nfrom geometry_msgs.msg import PoseStamped, Twist\nfrom sensor_msgs.msg import LaserScan, Image, Imu\nfrom action_msgs.msg import GoalStatus\nfrom rclpy.action import ActionClient\nfrom rclpy.callback_groups import ReentrantCallbackGroup\nfrom rclpy.qos import QoSProfile, QoSDurabilityPolicy, QoSHistoryPolicy, QoSReliabilityPolicy\nfrom threading import Lock\nimport asyncio\nimport json\nfrom enum import Enum\nfrom typing import Dict, Any, Optional, List\n\nclass SystemState(Enum):\n    IDLE = 0\n    LISTENING = 1\n    PLANNING = 2\n    EXECUTING = 3\n    RECOVERING = 4\n    EMERGENCY_STOP = 5\n\nclass SystemOrchestratorNode(Node):\n    def __init__(self):\n        super().__init__(\'system_orchestrator\')\n\n        # System state management\n        self.state = SystemState.IDLE\n        self.state_lock = Lock()\n\n        # Configuration\n        self.declare_parameter(\'operation_mode\', \'autonomous\')\n        self.declare_parameter(\'safety_timeout\', 30.0)\n        self.declare_parameter(\'max_velocity\', 0.5)\n\n        self.operation_mode = self.get_parameter(\'operation_mode\').value\n        self.safety_timeout = self.get_parameter(\'safety_timeout\').value\n        self.max_velocity = self.get_parameter(\'max_velocity\').value\n\n        # QoS profiles for different data types\n        self.sensor_qos = QoSProfile(\n            depth=10,\n            durability=QoSDurabilityPolicy.VOLATILE,\n            history=QoSHistoryPolicy.KEEP_LAST,\n            reliability=QoSReliabilityPolicy.BEST_EFFORT\n        )\n\n        self.command_qos = QoSProfile(\n            depth=10,\n            durability=QoSDurabilityPolicy.VOLATILE,\n            history=QoSHistoryPolicy.KEEP_LAST,\n            reliability=QoSReliabilityPolicy.RELIABLE\n        )\n\n        # Publishers\n        self.status_publisher = self.create_publisher(String, \'system_status\', self.command_qos)\n        self.cmd_vel_publisher = self.create_publisher(Twist, \'cmd_vel\', self.command_qos)\n        self.emergency_stop_publisher = self.create_publisher(Bool, \'emergency_stop\', self.command_qos)\n\n        # Subscriptions\n        self.voice_command_subscription = self.create_subscription(\n            String,\n            \'voice_command\',\n            self.voice_command_callback,\n            self.command_qos\n        )\n\n        self.high_level_goal_subscription = self.create_subscription(\n            String,\n            \'high_level_goal\',\n            self.high_level_goal_callback,\n            self.command_qos\n        )\n\n        self.sensor_status_subscription = self.create_subscription(\n            String,\n            \'sensor_status\',\n            self.sensor_status_callback,\n            self.command_qos\n        )\n\n        self.planning_status_subscription = self.create_subscription(\n            String,\n            \'planning_status\',\n            self.planning_status_callback,\n            self.command_qos\n        )\n\n        self.execution_status_subscription = self.create_subscription(\n            String,\n            \'execution_status\',\n            self.execution_status_callback,\n            self.command_qos\n        )\n\n        # Safety monitoring\n        self.laser_subscription = self.create_subscription(\n            LaserScan,\n            \'scan\',\n            self.laser_callback,\n            self.sensor_qos\n        )\n\n        self.imu_subscription = self.create_subscription(\n            Imu,\n            \'imu/data\',\n            self.imu_callback,\n            self.sensor_qos\n        )\n\n        # Action clients for coordination\n        self.navigation_client = ActionClient(self, \'nav2_msgs.action.NavigateToPose\', \'navigate_to_pose\')\n        self.manipulation_client = ActionClient(self, \'control_msgs.action.FollowJointTrajectory\', \'arm_controller/follow_joint_trajectory\')\n\n        # System state\n        self.current_goal = None\n        self.active_plan = None\n        self.emergency_stop_active = False\n        self.last_sensor_update = self.get_clock().now()\n        self.safety_timer = self.create_timer(1.0, self.safety_check)\n\n        # Async execution\n        self.executor = asyncio.get_event_loop()\n\n        self.get_logger().info(\'System Orchestrator initialized\')\n\n    def update_state(self, new_state: SystemState):\n        """Safely update system state"""\n        with self.state_lock:\n            old_state = self.state\n            self.state = new_state\n            self.get_logger().info(f\'System state changed: {old_state.name} -> {new_state.name}\')\n\n            # Publish state change\n            status_msg = String()\n            status_msg.data = f\'STATE_CHANGE: {new_state.name}\'\n            self.status_publisher.publish(status_msg)\n\n    def voice_command_callback(self, msg: String):\n        """Handle voice commands"""\n        if self.state == SystemState.EMERGENCY_STOP:\n            self.get_logger().warn(\'Ignoring voice command during emergency stop\')\n            return\n\n        command = msg.data.lower().strip()\n        self.get_logger().info(f\'Received voice command: {command}\')\n\n        # Process command based on current state\n        if self.state == SystemState.IDLE:\n            if any(word in command for word in [\'start\', \'begin\', \'go\', \'execute\']):\n                self.start_task_from_voice(command)\n            elif \'stop\' in command or \'halt\' in command:\n                self.update_state(SystemState.IDLE)\n            else:\n                self.process_high_level_command(command)\n        elif self.state == SystemState.LISTENING:\n            self.process_high_level_command(command)\n\n    def high_level_goal_callback(self, msg: String):\n        """Handle high-level goals"""\n        goal = msg.data\n        self.get_logger().info(f\'Received high-level goal: {goal}\')\n\n        if self.state in [SystemState.IDLE, SystemState.LISTENING]:\n            self.process_high_level_command(goal)\n\n    def process_high_level_command(self, command: str):\n        """Process high-level command and initiate planning"""\n        if self.emergency_stop_active:\n            self.get_logger().warn(\'Cannot process command during emergency stop\')\n            return\n\n        self.current_goal = command\n        self.update_state(SystemState.PLANNING)\n\n        # Send goal to cognitive planner\n        goal_msg = String()\n        goal_msg.data = command\n\n        # This would typically go to the cognitive planning node\n        # For this example, we\'ll simulate the planning process\n        self.simulate_planning_process(command)\n\n    def simulate_planning_process(self, goal: str):\n        """Simulate the planning process"""\n        # In a real system, this would call the cognitive planning node\n        # For simulation, create a mock plan\n        mock_plan = [\n            {"action": "navigate", "target": {"x": 1.0, "y": 1.0, "theta": 0.0}, "description": "Move to location"},\n            {"action": "perceive", "target": "object", "description": "Look for object"},\n            {"action": "manipulate", "target": "object", "description": "Pick up object"}\n        ]\n\n        self.active_plan = mock_plan\n        self.get_logger().info(f\'Generated plan with {len(mock_plan)} steps\')\n\n        # Transition to execution\n        self.update_state(SystemState.EXECUTING)\n        self.execute_plan()\n\n    def execute_plan(self):\n        """Execute the active plan"""\n        if not self.active_plan:\n            self.get_logger().error(\'No active plan to execute\')\n            self.update_state(SystemState.IDLE)\n            return\n\n        self.get_logger().info(f\'Executing plan with {len(self.active_plan)} steps\')\n\n        # Execute each step in the plan\n        for step in self.active_plan:\n            if self.state != SystemState.EXECUTING:\n                break  # Plan execution interrupted\n\n            success = self.execute_plan_step(step)\n            if not success:\n                self.get_logger().error(f\'Plan execution failed at step: {step}\')\n                self.update_state(SystemState.RECOVERING)\n                self.attempt_recovery()\n                break\n\n        if self.state == SystemState.EXECUTING:\n            self.get_logger().info(\'Plan execution completed successfully\')\n            self.update_state(SystemState.IDLE)\n\n    def execute_plan_step(self, step: Dict[str, Any]) -> bool:\n        """Execute a single plan step"""\n        action = step.get(\'action\', \'\')\n        target = step.get(\'target\', {})\n\n        self.get_logger().info(f\'Executing step: {action} - {step.get("description", "")}\')\n\n        if action == \'navigate\':\n            return self.execute_navigation_step(target)\n        elif action == \'perceive\':\n            return self.execute_perception_step(target)\n        elif action == \'manipulate\':\n            return self.execute_manipulation_step(target)\n        else:\n            self.get_logger().warn(f\'Unknown action: {action}\')\n            return False\n\n    def execute_navigation_step(self, target: Dict[str, Any]) -> bool:\n        """Execute navigation step"""\n        # Create navigation goal\n        from geometry_msgs.msg import PoseStamped\n        from nav2_msgs.action import NavigateToPose\n\n        goal_msg = NavigateToPose.Goal()\n        goal_msg.pose.header.frame_id = \'map\'\n        goal_msg.pose.pose.position.x = float(target.get(\'x\', 0.0))\n        goal_msg.pose.pose.position.y = float(target.get(\'y\', 0.0))\n        goal_msg.pose.pose.position.z = 0.0\n\n        # Convert theta to quaternion\n        from math import sin, cos\n        theta = target.get(\'theta\', 0.0)\n        goal_msg.pose.pose.orientation.z = sin(theta / 2.0)\n        goal_msg.pose.pose.orientation.w = cos(theta / 2.0)\n\n        # Send navigation goal\n        if self.navigation_client.wait_for_server(timeout_sec=5.0):\n            future = self.navigation_client.send_goal_async(goal_msg)\n            # Wait for result (in real system, this would be async)\n            return True\n        else:\n            self.get_logger().error(\'Navigation server not available\')\n            return False\n\n    def execute_perception_step(self, target: Dict[str, Any]) -> bool:\n        """Execute perception step"""\n        # This would trigger perception nodes\n        # For this example, return success\n        self.get_logger().info(f\'Performing perception for: {target}\')\n        return True\n\n    def execute_manipulation_step(self, target: Dict[str, Any]) -> bool:\n        """Execute manipulation step"""\n        # This would trigger manipulation nodes\n        # For this example, return success\n        self.get_logger().info(f\'Performing manipulation for: {target}\')\n        return True\n\n    def attempt_recovery(self):\n        """Attempt to recover from execution failure"""\n        self.get_logger().info(\'Attempting recovery from execution failure\')\n\n        # Recovery strategies could include:\n        # - Returning to a safe location\n        # - Requesting human assistance\n        # - Trying alternative approaches\n\n        # For this example, return to idle state\n        self.update_state(SystemState.IDLE)\n\n    def sensor_status_callback(self, msg: String):\n        """Handle sensor status updates"""\n        self.last_sensor_update = self.get_clock().now()\n\n        # Update sensor status in system state\n        # This could trigger different behaviors based on sensor health\n        if \'error\' in msg.data.lower():\n            self.get_logger().warn(f\'Sensor error: {msg.data}\')\n            if self.state == SystemState.EXECUTING:\n                self.update_state(SystemState.RECOVERING)\n\n    def planning_status_callback(self, msg: String):\n        """Handle planning status updates"""\n        if \'completed\' in msg.data.lower() and self.state == SystemState.PLANNING:\n            self.update_state(SystemState.EXECUTING)\n        elif \'failed\' in msg.data.lower():\n            self.get_logger().error(f\'Planning failed: {msg.data}\')\n            self.update_state(SystemState.IDLE)\n\n    def execution_status_callback(self, msg: String):\n        """Handle execution status updates"""\n        if \'completed\' in msg.data.lower() and self.state == SystemState.EXECUTING:\n            self.update_state(SystemState.IDLE)\n        elif \'failed\' in msg.data.lower():\n            self.get_logger().error(f\'Execution failed: {msg.data}\')\n            self.update_state(SystemState.RECOVERING)\n\n    def laser_callback(self, msg: LaserScan):\n        """Handle laser scan for safety monitoring"""\n        if self.emergency_stop_active:\n            return\n\n        # Check for obstacles in path\n        min_distance = min([r for r in msg.ranges if 0 < r < float(\'inf\')], default=float(\'inf\'))\n\n        if min_distance < 0.5:  # 50cm safety margin\n            self.get_logger().warn(f\'Obstacle detected at {min_distance:.2f}m, initiating safety stop\')\n            self.activate_emergency_stop()\n\n    def imu_callback(self, msg: Imu):\n        """Handle IMU data for balance monitoring"""\n        if self.emergency_stop_active:\n            return\n\n        # Check for dangerous tilt angles\n        # Convert quaternion to roll/pitch\n        import math\n        sinr_cosp = 2 * (msg.orientation.w * msg.orientation.x + msg.orientation.y * msg.orientation.z)\n        cosr_cosp = 1 - 2 * (msg.orientation.x * msg.orientation.x + msg.orientation.y * msg.orientation.y)\n        roll = math.atan2(sinr_cosp, cosr_cosp)\n\n        sinp = 2 * (msg.orientation.w * msg.orientation.y - msg.orientation.z * msg.orientation.x)\n        if abs(sinp) >= 1:\n            pitch = math.copysign(math.pi / 2, sinp)\n        else:\n            pitch = math.asin(sinp)\n\n        # Check if tilt exceeds safe limits\n        max_tilt = 0.5  # 0.5 radians ~ 28 degrees\n        if abs(roll) > max_tilt or abs(pitch) > max_tilt:\n            self.get_logger().warn(f\'Dangerous tilt detected: roll={roll:.2f}, pitch={pitch:.2f}\')\n            self.activate_emergency_stop()\n\n    def safety_check(self):\n        """Periodic safety check"""\n        if self.emergency_stop_active:\n            return\n\n        # Check if sensor data is stale\n        time_since_sensor_update = (self.get_clock().now() - self.last_sensor_update).nanoseconds / 1e9\n        if time_since_sensor_update > self.safety_timeout:\n            self.get_logger().error(f\'Sensor data stale for {time_since_sensor_update:.1f}s, initiating safety stop\')\n            self.activate_emergency_stop()\n\n        # Check system health\n        if self.state == SystemState.EXECUTING:\n            # Additional checks during execution\n            pass\n\n    def activate_emergency_stop(self):\n        """Activate emergency stop"""\n        if not self.emergency_stop_active:\n            self.emergency_stop_active = True\n            self.get_logger().fatal(\'EMERGENCY STOP ACTIVATED\')\n\n            # Stop all movement\n            stop_msg = Twist()\n            self.cmd_vel_publisher.publish(stop_msg)\n\n            # Publish emergency stop signal\n            emergency_msg = Bool()\n            emergency_msg.data = True\n            self.emergency_stop_publisher.publish(emergency_msg)\n\n            # Change state to emergency stop\n            self.update_state(SystemState.EMERGENCY_STOP)\n\n    def start_task_from_voice(self, command: str):\n        """Start a task based on voice command"""\n        if self.state == SystemState.EMERGENCY_STOP:\n            self.get_logger().warn(\'Cannot start task during emergency stop\')\n            return\n\n        # Parse command and start appropriate task\n        if any(word in command for word in [\'navigate\', \'go to\', \'move to\']):\n            # Extract destination from command\n            self.process_navigation_command(command)\n        elif any(word in command for word in [\'pick\', \'grasp\', \'take\']):\n            # Extract object from command\n            self.process_manipulation_command(command)\n        elif any(word in command for word in [\'follow\', \'track\']):\n            # Start following behavior\n            self.process_follow_command(command)\n        else:\n            # Send to cognitive planner for complex reasoning\n            self.process_high_level_command(command)\n\n    def process_navigation_command(self, command: str):\n        """Process navigation command"""\n        # In a real system, this would extract location from command\n        # For this example, use a default location\n        self.get_logger().info(f\'Processing navigation command: {command}\')\n        self.process_high_level_command(f"navigate to {command}")\n\n    def process_manipulation_command(self, command: str):\n        """Process manipulation command"""\n        self.get_logger().info(f\'Processing manipulation command: {command}\')\n        self.process_high_level_command(f"manipulate {command}")\n\n    def process_follow_command(self, command: str):\n        """Process follow command"""\n        self.get_logger().info(f\'Processing follow command: {command}\')\n        self.process_high_level_command(f"follow {command}")\n\n    def start_system(self):\n        """Start the autonomous system"""\n        if self.state == SystemState.IDLE:\n            self.get_logger().info(\'Starting autonomous humanoid system\')\n            self.update_state(SystemState.LISTENING)\n\n    def stop_system(self):\n        """Stop the autonomous system"""\n        self.get_logger().info(\'Stopping autonomous humanoid system\')\n        self.update_state(SystemState.IDLE)\n\n        # Stop all movement\n        stop_msg = Twist()\n        self.cmd_vel_publisher.publish(stop_msg)\n\n    def reset_system(self):\n        """Reset the system to initial state"""\n        self.emergency_stop_active = False\n        self.current_goal = None\n        self.active_plan = None\n        self.update_state(SystemState.IDLE)\n\n        self.get_logger().info(\'System reset complete\')\n'})}),"\n",(0,o.jsx)(n.h2,{id:"integration-with-simulation-environments",children:"Integration with Simulation Environments"}),"\n",(0,o.jsx)(n.h3,{id:"isaac-sim-integration",children:"Isaac Sim Integration"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'# Example of integrating with Isaac Sim for training and testing\nimport rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String, Bool\nfrom sensor_msgs.msg import Image, LaserScan\nfrom geometry_msgs.msg import Twist\nfrom nav_msgs.msg import Odometry\nimport numpy as np\n\nclass IsaacSimIntegrationNode(Node):\n    def __init__(self):\n        super().__init__(\'isaac_sim_integration\')\n\n        # Publishers for Isaac Sim\n        self.sim_control_publisher = self.create_publisher(Bool, \'/isaac_sim/control\', 10)\n        self.sim_reset_publisher = self.create_publisher(Bool, \'/isaac_sim/reset\', 10)\n\n        # Subscriptions from Isaac Sim\n        self.sim_odom_subscription = self.create_subscription(\n            Odometry,\n            \'/isaac_sim/ground_truth/odometry\',\n            self.sim_odom_callback,\n            10\n        )\n\n        self.sim_image_subscription = self.create_subscription(\n            Image,\n            \'/isaac_sim/camera/image\',\n            self.sim_image_callback,\n            10\n        )\n\n        # Integration with main system\n        self.system_status_subscription = self.create_subscription(\n            String,\n            \'system_status\',\n            self.system_status_callback,\n            10\n        )\n\n        self.cmd_vel_subscription = self.create_subscription(\n            Twist,\n            \'cmd_vel\',\n            self.cmd_vel_callback,\n            10\n        )\n\n        # Simulation parameters\n        self.declare_parameter(\'sim_enabled\', True)\n        self.declare_parameter(\'sim_rate\', 100.0)  # Hz\n        self.declare_parameter(\'sim_reset_on_failure\', True)\n\n        self.sim_enabled = self.get_parameter(\'sim_enabled\').value\n        self.sim_rate = self.get_parameter(\'sim_rate\').value\n        self.sim_reset_on_failure = self.get_parameter(\'sim_reset_on_failure\').value\n\n        # Simulation state\n        self.sim_robot_pose = None\n        self.sim_last_reset = self.get_clock().now()\n\n        # Timer for simulation synchronization\n        self.sim_timer = self.create_timer(1.0/self.sim_rate, self.sim_sync_callback)\n\n        self.get_logger().info(\'Isaac Sim integration node initialized\')\n\n    def system_status_callback(self, msg: String):\n        """Handle system status updates for simulation"""\n        if \'EMERGENCY_STOP\' in msg.data and self.sim_reset_on_failure:\n            self.reset_simulation()\n\n    def cmd_vel_callback(self, msg: Twist):\n        """Forward velocity commands to simulation"""\n        if self.sim_enabled:\n            # In a real integration, this would send commands to Isaac Sim\n            # For this example, log the command\n            self.get_logger().debug(f\'Sim cmd_vel: linear={msg.linear.x}, angular={msg.angular.z}\')\n\n    def sim_odom_callback(self, msg: Odometry):\n        """Handle simulation odometry"""\n        self.sim_robot_pose = msg.pose.pose\n\n    def sim_image_callback(self, msg: Image):\n        """Handle simulation camera images"""\n        # Process simulation images\n        # This could be used for synthetic data generation\n        pass\n\n    def sim_sync_callback(self):\n        """Synchronize with simulation"""\n        if not self.sim_enabled:\n            return\n\n        # Perform any necessary synchronization tasks\n        # Check simulation health\n        pass\n\n    def reset_simulation(self):\n        """Reset the simulation environment"""\n        if self.sim_enabled:\n            reset_msg = Bool()\n            reset_msg.data = True\n            self.sim_reset_publisher.publish(reset_msg)\n            self.sim_last_reset = self.get_clock().now()\n            self.get_logger().info(\'Simulation reset triggered\')\n\n    def configure_simulation_environment(self, env_config: dict):\n        """Configure simulation environment"""\n        # This would configure Isaac Sim with specific parameters\n        # For example: environment layout, object placement, lighting\n        pass\n'})}),"\n",(0,o.jsx)(n.h2,{id:"real-world-deployment-considerations",children:"Real-World Deployment Considerations"}),"\n",(0,o.jsx)(n.h3,{id:"hardware-abstraction-layer",children:"Hardware Abstraction Layer"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"# Hardware abstraction for real-world deployment\nimport rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nfrom sensor_msgs.msg import JointState, Imu\nfrom control_msgs.msg import JointTrajectoryControllerState\nfrom builtin_interfaces.msg import Duration\nimport yaml\n\nclass HardwareAbstractionNode(Node):\n    def __init__(self):\n        super().__init__('hardware_abstraction')\n\n        # Load hardware configuration\n        self.hardware_config = self.load_hardware_config()\n\n        # Initialize hardware interfaces based on configuration\n        self.initialize_hardware_interfaces()\n\n        # Publishers and subscriptions for hardware status\n        self.hardware_status_publisher = self.create_publisher(String, 'hardware_status', 10)\n\n        # Hardware state monitoring\n        self.joint_state_subscription = self.create_subscription(\n            JointState,\n            'joint_states',\n            self.joint_state_callback,\n            10\n        )\n\n        self.imu_subscription = self.create_subscription(\n            Imu,\n            'imu/data',\n            self.imu_callback,\n            10\n        )\n\n        # Timer for hardware health checks\n        self.health_check_timer = self.create_timer(1.0, self.hardware_health_check)\n\n        self.get_logger().info('Hardware abstraction layer initialized')\n\n    def load_hardware_config(self) -> dict:\n        \"\"\"Load hardware configuration from file\"\"\"\n        # In a real system, this would load from a config file\n        # For this example, return a mock configuration\n        return {\n            'joints': {\n                'head_pan': {'type': 'revolute', 'limits': {'min': -1.57, 'max': 1.57}},\n                'head_tilt': {'type': 'revolute', 'limits': {'min': -0.78, 'max': 0.78}},\n                'left_hip': {'type': 'revolute', 'limits': {'min': -1.57, 'max': 1.57}},\n                'left_knee': {'type': 'revolute', 'limits': {'min': 0.0, 'max': 1.57}},\n                'left_ankle': {'type': 'revolute', 'limits': {'min': -0.78, 'max': 0.78}},\n                'right_hip': {'type': 'revolute', 'limits': {'min': -1.57, 'max': 1.57}},\n                'right_knee': {'type': 'revolute', 'limits': {'min': 0.0, 'max': 1.57}},\n                'right_ankle': {'type': 'revolute', 'limits': {'min': -0.78, 'max': 0.78}},\n                'left_shoulder': {'type': 'revolute', 'limits': {'min': -1.57, 'max': 1.57}},\n                'left_elbow': {'type': 'revolute', 'limits': {'min': -1.57, 'max': 0.0}},\n                'right_shoulder': {'type': 'revolute', 'limits': {'min': -1.57, 'max': 1.57}},\n                'right_elbow': {'type': 'revolute', 'limits': {'min': -1.57, 'max': 0.0}},\n            },\n            'sensors': {\n                'imu': {'type': 'imu', 'rate': 100},\n                'camera': {'type': 'rgb', 'rate': 30},\n                'lidar': {'type': 'laser', 'rate': 10},\n                'force_torque': {'type': 'force_torque', 'rate': 100}\n            },\n            'actuators': {\n                'head_pan': {'type': 'servo', 'max_speed': 1.0},\n                'head_tilt': {'type': 'servo', 'max_speed': 1.0},\n                # Add other actuators...\n            }\n        }\n\n    def initialize_hardware_interfaces(self):\n        \"\"\"Initialize hardware interfaces based on configuration\"\"\"\n        # Initialize each joint controller\n        for joint_name, joint_config in self.hardware_config['joints'].items():\n            self.get_logger().info(f'Initializing joint: {joint_name}')\n\n        # Initialize sensors\n        for sensor_name, sensor_config in self.hardware_config['sensors'].items():\n            self.get_logger().info(f'Initializing sensor: {sensor_name}')\n\n    def joint_state_callback(self, msg: JointState):\n        \"\"\"Monitor joint states for hardware health\"\"\"\n        # Check joint limits and safety\n        for i, name in enumerate(msg.name):\n            if name in self.hardware_config['joints']:\n                position = msg.position[i]\n                limits = self.hardware_config['joints'][name]['limits']\n\n                if position < limits['min'] or position > limits['max']:\n                    self.get_logger().warn(f'Joint {name} out of limits: {position}')\n\n    def imu_callback(self, msg: Imu):\n        \"\"\"Monitor IMU data for hardware health\"\"\"\n        # Check IMU data validity\n        if abs(msg.linear_acceleration.x) > 20.0:  # Check for unrealistic values\n            self.get_logger().warn('Unrealistic IMU acceleration values')\n\n    def hardware_health_check(self):\n        \"\"\"Perform periodic hardware health checks\"\"\"\n        # Check all hardware components\n        status = self.check_all_hardware()\n\n        status_msg = String()\n        status_msg.data = f\"Hardware Status: {status['overall']}, Components: {status['details']}\"\n        self.hardware_status_publisher.publish(status_msg)\n\n    def check_all_hardware(self) -> dict:\n        \"\"\"Check health of all hardware components\"\"\"\n        # This would perform comprehensive hardware checks\n        # For this example, return a mock status\n        return {\n            'overall': 'OK',\n            'details': {\n                'joints': 'OK',\n                'sensors': 'OK',\n                'actuators': 'OK',\n                'power': 'OK'\n            }\n        }\n\n    def send_joint_commands(self, joint_positions: dict, duration: float = 1.0):\n        \"\"\"Send commands to joints with hardware abstraction\"\"\"\n        # Validate commands against hardware limits\n        validated_positions = {}\n        for joint, position in joint_positions.items():\n            if joint in self.hardware_config['joints']:\n                limits = self.hardware_config['joints'][joint]['limits']\n                clamped_pos = max(limits['min'], min(limits['max'], position))\n                validated_positions[joint] = clamped_pos\n            else:\n                self.get_logger().warn(f'Unknown joint: {joint}')\n\n        # Send validated commands to hardware interface\n        # This would interface with the actual hardware controller\n        self.get_logger().info(f'Sending joint commands: {validated_positions}')\n"})}),"\n",(0,o.jsx)(n.h2,{id:"performance-monitoring-and-optimization",children:"Performance Monitoring and Optimization"}),"\n",(0,o.jsx)(n.h3,{id:"system-performance-node",children:"System Performance Node"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"# Performance monitoring for the autonomous system\nimport rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nfrom diagnostic_msgs.msg import DiagnosticArray, DiagnosticStatus\nfrom rclpy.qos import QoSProfile\nimport psutil\nimport time\nfrom collections import deque\nimport threading\n\nclass PerformanceMonitorNode(Node):\n    def __init__(self):\n        super().__init__('performance_monitor')\n\n        # Publishers\n        self.diag_publisher = self.create_publisher(DiagnosticArray, '/diagnostics', 10)\n        self.performance_publisher = self.create_publisher(String, 'system_performance', 10)\n\n        # Performance tracking\n        self.cpu_usage_history = deque(maxlen=100)\n        self.memory_usage_history = deque(maxlen=100)\n        self.process_count_history = deque(maxlen=100)\n\n        # Component performance tracking\n        self.component_times = {}\n        self.component_calls = {}\n\n        # Monitoring timer\n        self.monitor_timer = self.create_timer(1.0, self.monitor_system)\n\n        # Threading for non-blocking monitoring\n        self.monitoring_thread = threading.Thread(target=self.background_monitoring)\n        self.monitoring_thread.daemon = True\n        self.monitoring_thread.start()\n\n        self.get_logger().info('Performance monitor initialized')\n\n    def monitor_system(self):\n        \"\"\"Monitor system performance\"\"\"\n        # Get system metrics\n        cpu_percent = psutil.cpu_percent(interval=1)\n        memory_percent = psutil.virtual_memory().percent\n        process_count = len(psutil.pids())\n\n        # Store in history\n        self.cpu_usage_history.append(cpu_percent)\n        self.memory_usage_history.append(memory_percent)\n        self.process_count_history.append(process_count)\n\n        # Create diagnostic message\n        diag_array = DiagnosticArray()\n        diag_array.header.stamp = self.get_clock().now().to_msg()\n\n        # CPU diagnostic\n        cpu_diag = DiagnosticStatus()\n        cpu_diag.name = 'System CPU Usage'\n        cpu_diag.level = DiagnosticStatus.OK if cpu_percent < 80 else DiagnosticStatus.WARN if cpu_percent < 90 else DiagnosticStatus.ERROR\n        cpu_diag.message = f'CPU usage: {cpu_percent}%'\n        cpu_diag.hardware_id = 'system'\n        cpu_diag.values = [\n            {'key': 'CPU %', 'value': str(cpu_percent)},\n            {'key': 'Status', 'value': 'OK' if cpu_percent < 80 else 'WARNING' if cpu_percent < 90 else 'ERROR'}\n        ]\n        diag_array.status.append(cpu_diag)\n\n        # Memory diagnostic\n        memory_diag = DiagnosticStatus()\n        memory_diag.name = 'System Memory Usage'\n        memory_diag.level = DiagnosticStatus.OK if memory_percent < 80 else DiagnosticStatus.WARN if memory_percent < 90 else DiagnosticStatus.ERROR\n        memory_diag.message = f'Memory usage: {memory_percent}%'\n        memory_diag.hardware_id = 'system'\n        memory_diag.values = [\n            {'key': 'Memory %', 'value': str(memory_percent)},\n            {'key': 'Status', 'value': 'OK' if memory_percent < 80 else 'WARNING' if memory_percent < 90 else 'ERROR'}\n        ]\n        diag_array.status.append(memory_diag)\n\n        # Publish diagnostics\n        self.diag_publisher.publish(diag_array)\n\n        # Log performance if exceeding thresholds\n        if cpu_percent > 85 or memory_percent > 85:\n            self.get_logger().warn(f'High resource usage - CPU: {cpu_percent}%, Memory: {memory_percent}%')\n\n    def background_monitoring(self):\n        \"\"\"Background monitoring thread\"\"\"\n        while rclpy.ok():\n            try:\n                # Perform background monitoring tasks\n                # This could include network monitoring, disk usage, etc.\n                time.sleep(0.1)  # Prevent busy waiting\n            except Exception as e:\n                self.get_logger().error(f'Background monitoring error: {e}')\n\n    def track_component_performance(self, component_name: str, execution_time: float):\n        \"\"\"Track performance of specific components\"\"\"\n        if component_name not in self.component_times:\n            self.component_times[component_name] = deque(maxlen=50)\n            self.component_calls[component_name] = 0\n\n        self.component_times[component_name].append(execution_time)\n        self.component_calls[component_name] += 1\n\n        # Log if component is slow\n        avg_time = sum(self.component_times[component_name]) / len(self.component_times[component_name])\n        if avg_time > 0.1:  # More than 100ms average\n            self.get_logger().warn(f'Slow component {component_name}: avg {avg_time:.3f}s')\n\n    def get_performance_report(self) -> str:\n        \"\"\"Generate performance report\"\"\"\n        report = {\n            'system': {\n                'cpu_avg': sum(self.cpu_usage_history) / len(self.cpu_usage_history) if self.cpu_usage_history else 0,\n                'memory_avg': sum(self.memory_usage_history) / len(self.memory_usage_history) if self.memory_usage_history else 0,\n                'processes_avg': sum(self.process_count_history) / len(self.process_count_history) if self.process_count_history else 0\n            },\n            'components': {}\n        }\n\n        for comp, times in self.component_times.items():\n            if times:\n                avg_time = sum(times) / len(times)\n                report['components'][comp] = {\n                    'avg_time': avg_time,\n                    'call_count': self.component_calls[comp],\n                    'max_time': max(times)\n                }\n\n        return str(report)\n\n    def publish_performance_report(self):\n        \"\"\"Publish performance report\"\"\"\n        report = self.get_performance_report()\n        report_msg = String()\n        report_msg.data = report\n        self.performance_publisher.publish(report_msg)\n"})}),"\n",(0,o.jsx)(n.h2,{id:"testing-and-validation-framework",children:"Testing and Validation Framework"}),"\n",(0,o.jsx)(n.h3,{id:"system-integration-tests",children:"System Integration Tests"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'# Integration testing for the complete autonomous humanoid system\nimport unittest\nimport rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nfrom geometry_msgs.msg import Twist\nfrom sensor_msgs.msg import LaserScan\nfrom rclpy.action import ActionClient\nfrom rclpy.qos import QoSProfile\nimport time\nimport threading\n\nclass AutonomousHumanoidTestSuite(unittest.TestCase):\n    def setUp(self):\n        """Set up test environment"""\n        rclpy.init()\n        self.node = TestNode(\'autonomous_humanoid_tester\')\n\n        # Create clients and subscribers for testing\n        self.status_subscriber = self.node.create_subscription(\n            String, \'system_status\', self.status_callback, 10\n        )\n\n        self.cmd_vel_publisher = self.node.create_publisher(Twist, \'cmd_vel\', 10)\n        self.goal_publisher = self.node.create_publisher(String, \'high_level_goal\', 10)\n\n        self.current_status = None\n        self.status_received = threading.Event()\n\n    def status_callback(self, msg):\n        """Callback for system status"""\n        self.current_status = msg.data\n        self.status_received.set()\n\n    def test_basic_navigation(self):\n        """Test basic navigation functionality"""\n        # Publish a simple navigation goal\n        goal_msg = String()\n        goal_msg.data = "go to kitchen"\n        self.goal_publisher.publish(goal_msg)\n\n        # Wait for system to change state\n        self.status_received.wait(timeout=10.0)\n\n        # Check that system is in planning or executing state\n        self.assertIsNotNone(self.current_status)\n        self.assertIn("STATE_CHANGE", self.current_status)\n\n    def test_voice_command_processing(self):\n        """Test voice command processing"""\n        # Simulate voice command\n        voice_cmd_publisher = self.node.create_publisher(String, \'voice_command\', 10)\n\n        cmd_msg = String()\n        cmd_msg.data = "move forward slowly"\n        voice_cmd_publisher.publish(cmd_msg)\n\n        # Check for appropriate response\n        self.status_received.wait(timeout=5.0)\n        self.assertIsNotNone(self.current_status)\n\n    def test_emergency_stop(self):\n        """Test emergency stop functionality"""\n        # Publish emergency stop command\n        # This would be tested with simulated sensor data\n        pass\n\n    def test_sensor_integration(self):\n        """Test sensor data integration"""\n        # Simulate sensor data\n        scan_publisher = self.node.create_publisher(LaserScan, \'scan\', 10)\n\n        scan_msg = LaserScan()\n        scan_msg.ranges = [1.0] * 360  # Simulated laser scan\n        scan_msg.angle_min = -3.14\n        scan_msg.angle_max = 3.14\n        scan_msg.angle_increment = 3.14 * 2 / 360\n        scan_publisher.publish(scan_msg)\n\n        # Verify system processes sensor data\n        time.sleep(0.1)  # Allow processing time\n        # Add assertions based on expected behavior\n\n    def tearDown(self):\n        """Clean up after tests"""\n        self.node.destroy_node()\n        rclpy.shutdown()\n\nclass TestNode(Node):\n    def __init__(self, name):\n        super().__init__(name)\n\ndef run_integration_tests():\n    """Run the complete integration test suite"""\n    test_suite = unittest.TestLoader().loadTestsFromTestCase(AutonomousHumanoidTestSuite)\n    test_runner = unittest.TextTestRunner(verbosity=2)\n    result = test_runner.run(test_suite)\n\n    return result.wasSuccessful()\n\nif __name__ == \'__main__\':\n    success = run_integration_tests()\n    exit(0 if success else 1)\n'})}),"\n",(0,o.jsx)(n.h2,{id:"deployment-and-maintenance",children:"Deployment and Maintenance"}),"\n",(0,o.jsx)(n.h3,{id:"system-deployment-scripts",children:"System Deployment Scripts"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-bash",children:'#!/bin/bash\n# Deployment script for autonomous humanoid system\n\nset -e  # Exit on any error\n\n# Configuration\nROS_DISTRO="humble"\nSYSTEM_NAME="autonomous_humanoid"\nUSER_NAME=$(whoami)\n\necho "Deploying Autonomous Humanoid System..."\n\n# Check prerequisites\nif ! command -v ros2 &> /dev/null; then\n    echo "ROS 2 is not installed. Please install ROS 2 ${ROS_DISTRO} first."\n    exit 1\nfi\n\nif ! command -v docker &> /dev/null; then\n    echo "Docker is not installed. Installing Docker..."\n    curl -fsSL https://get.docker.com -o get-docker.sh\n    sh get-docker.sh\n    sudo usermod -aG docker $USER_NAME\nfi\n\n# Create workspace\nif [ ! -d "autonomous_humanoid_ws" ]; then\n    mkdir -p autonomous_humanoid_ws/src\n    cd autonomous_humanoid_ws\nelse\n    cd autonomous_humanoid_ws\nfi\n\n# Build the system\necho "Building autonomous humanoid system..."\ncolcon build --packages-select autonomous_humanoid --symlink-install\n\n# Source the workspace\nsource install/setup.bash\n\n# Install Python dependencies\npip3 install -r src/autonomous_humanoid/requirements.txt\n\n# Set up system services\necho "Setting up system services..."\n\n# Create systemd service for main system\ncat > /tmp/autonomous_humanoid.service << EOF\n[Unit]\nDescription=Autonomous Humanoid System\nAfter=network.target\n\n[Service]\nType=simple\nUser=$USER_NAME\nWorkingDirectory=/home/$USER_NAME/autonomous_humanoid_ws\nEnvironment="ROS_DOMAIN_ID=0"\nExecStart=/home/$USER_NAME/autonomous_humanoid_ws/install/autonomous_humanoid/lib/autonomous_humanoid/system_orchestrator\nRestart=always\nRestartSec=5\n\n[Install]\nWantedBy=multi-user.target\nEOF\n\nsudo cp /tmp/autonomous_humanoid.service /etc/systemd/system/\nsudo systemctl daemon-reload\nsudo systemctl enable autonomous_humanoid.service\n\necho "Deployment complete!"\necho "To start the system: sudo systemctl start autonomous_humanoid"\necho "To check status: sudo systemctl status autonomous_humanoid"\necho "To view logs: journalctl -u autonomous_humanoid -f"\n'})}),"\n",(0,o.jsx)(n.h2,{id:"troubleshooting-and-debugging",children:"Troubleshooting and Debugging"}),"\n",(0,o.jsx)(n.h3,{id:"common-issues-and-solutions",children:"Common Issues and Solutions"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'# Troubleshooting guide and diagnostic tools\nclass AutonomousHumanoidDiagnostics:\n    """\n    Diagnostic tools and troubleshooting guide for the autonomous humanoid system\n    """\n\n    @staticmethod\n    def check_ros_communication():\n        """Check ROS communication between nodes"""\n        import subprocess\n        try:\n            # Check if ROS master is running\n            result = subprocess.run([\'ros2\', \'node\', \'list\'], capture_output=True, text=True)\n            if result.returncode == 0:\n                nodes = result.stdout.strip().split(\'\\n\')\n                print(f"Active ROS nodes: {len(nodes)}")\n                for node in nodes:\n                    print(f"  - {node}")\n                return True\n            else:\n                print("ROS communication issue detected")\n                return False\n        except Exception as e:\n            print(f"Error checking ROS communication: {e}")\n            return False\n\n    @staticmethod\n    def check_sensor_data():\n        """Check if sensor data is being received properly"""\n        # This would check actual sensor topics\n        # For this example, return a mock check\n        print("Checking sensor data streams...")\n        print("\u2713 Camera stream: Active")\n        print("\u2713 LiDAR stream: Active")\n        print("\u2713 IMU stream: Active")\n        print("\u2713 Joint states: Active")\n        return True\n\n    @staticmethod\n    def check_llm_connection():\n        """Check LLM API connection"""\n        try:\n            import openai\n            # Test API key validity\n            client = openai.OpenAI()\n            # Make a simple test call\n            client.models.list()\n            print("\u2713 LLM API connection: Active")\n            return True\n        except Exception as e:\n            print(f"\u2717 LLM API connection: Failed - {e}")\n            return False\n\n    @staticmethod\n    def check_system_resources():\n        """Check system resource usage"""\n        import psutil\n        cpu_percent = psutil.cpu_percent(interval=1)\n        memory_percent = psutil.virtual_memory().percent\n        disk_percent = psutil.disk_usage(\'/\').percent\n\n        print(f"System Resources:")\n        print(f"  CPU: {cpu_percent}%")\n        print(f"  Memory: {memory_percent}%")\n        print(f"  Disk: {disk_percent}%")\n\n        if cpu_percent > 90 or memory_percent > 90:\n            print("\u26a0\ufe0f  High resource usage detected!")\n            return False\n        return True\n\n    @staticmethod\n    def run_comprehensive_diagnostics():\n        """Run comprehensive system diagnostics"""\n        print("Running comprehensive diagnostics...")\n        print("=" * 50)\n\n        checks = [\n            ("ROS Communication", AutonomousHumanoidDiagnostics.check_ros_communication),\n            ("Sensor Data", AutonomousHumanoidDiagnostics.check_sensor_data),\n            ("LLM Connection", AutonomousHumanoidDiagnostics.check_llm_connection),\n            ("System Resources", AutonomousHumanoidDiagnostics.check_system_resources),\n        ]\n\n        results = {}\n        for name, check_func in checks:\n            print(f"\\n{name}:")\n            try:\n                result = check_func()\n                results[name] = result\n                status = "\u2713 PASS" if result else "\u2717 FAIL"\n                print(f"  Status: {status}")\n            except Exception as e:\n                results[name] = False\n                print(f"  Status: \u2717 ERROR - {e}")\n\n        print("\\n" + "=" * 50)\n        print("Diagnostics Summary:")\n        passed = sum(1 for result in results.values() if result)\n        total = len(results)\n        print(f"Passed: {passed}/{total}")\n\n        if passed == total:\n            print("\u2713 All systems nominal!")\n        else:\n            print("\u26a0\ufe0f  Some systems require attention!")\n\n        return passed == total\n\n# Common troubleshooting commands\n"""\n# Check system status\nros2 launch autonomous_humanoid bringup_launch.py\n\n# View system logs\njournalctl -u autonomous_humanoid -f\n\n# Check network connectivity\nping -c 3 api.openai.com\n\n# Check sensor data\nros2 topic echo /scan --field ranges[0]\nros2 topic echo /camera/image_raw --field height\n\n# Check system resources\nhtop\ndf -h\nfree -h\n\n# Reset emergency stop\nros2 topic pub /emergency_stop std_msgs/Bool "{data: false}"\n\n# Run diagnostics\npython3 -c "from diagnostic_tools import AutonomousHumanoidDiagnostics; AutonomousHumanoidDiagnostics.run_comprehensive_diagnostics()"\n"""\n'})}),"\n",(0,o.jsx)(n.h2,{id:"rag-summary",children:"RAG Summary"}),"\n",(0,o.jsx)(n.p,{children:"The complete autonomous humanoid pipeline integrates all modules from the Physical AI & Humanoid Robotics book into a unified system. The architecture combines perception, cognition, and action layers with voice interaction and multimodal capabilities. The system includes comprehensive safety monitoring, performance optimization, and deployment considerations. The pipeline demonstrates the complete flow from voice commands to physical robot actions, incorporating ROS 2 fundamentals, simulation environments, perception systems, and Vision-Language-Action capabilities."}),"\n",(0,o.jsx)(n.h2,{id:"knowledge-check",children:"Knowledge Check"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsx)(n.li,{children:"What are the key components of the complete autonomous humanoid system?"}),"\n",(0,o.jsx)(n.li,{children:"How does the system orchestrator coordinate different subsystems?"}),"\n",(0,o.jsx)(n.li,{children:"What safety mechanisms are implemented in the autonomous system?"}),"\n",(0,o.jsx)(n.li,{children:"How is the system optimized for real-world deployment?"}),"\n",(0,o.jsx)(n.li,{children:"What diagnostic and troubleshooting tools are available?"}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"next-steps",children:"Next Steps"}),"\n",(0,o.jsx)(n.p,{children:"With the completion of this capstone chapter, you have learned the complete pipeline for autonomous humanoid robotics:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"ROS 2 fundamentals for robot communication"}),"\n",(0,o.jsx)(n.li,{children:"Simulation environments for testing and training"}),"\n",(0,o.jsx)(n.li,{children:"Perception systems for environment understanding"}),"\n",(0,o.jsx)(n.li,{children:"Cognitive planning with LLMs"}),"\n",(0,o.jsx)(n.li,{children:"Voice interaction with Whisper"}),"\n",(0,o.jsx)(n.li,{children:"Complete system integration and deployment"}),"\n"]}),"\n",(0,o.jsx)(n.p,{children:"This completes the Physical AI & Humanoid Robotics textbook, providing you with comprehensive knowledge to build and deploy autonomous humanoid robots."})]})}function u(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(c,{...e})}):c(e)}},8453:(e,n,t)=>{t.d(n,{R:()=>i});var s=t(6540);const o={},a=s.createContext(o);function i(e){const n=s.useContext(a);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}}}]);