"use strict";(globalThis.webpackChunkphysical_ai_humanoid_robotics=globalThis.webpackChunkphysical_ai_humanoid_robotics||[]).push([[353],{7515:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>r,contentTitle:()=>l,default:()=>u,frontMatter:()=>a,metadata:()=>o,toc:()=>d});const o=JSON.parse('{"id":"modules/module-4-vla/index","title":"Module 4: Vision-Language-Action (VLA)","description":"Welcome to Module 4 of the Physical AI & Humanoid Robotics book. This module focuses on Vision-Language-Action systems that enable humanoid robots to understand and respond to human commands through voice and visual input.","source":"@site/docs/modules/module-4-vla/index.md","sourceDirName":"modules/module-4-vla","slug":"/modules/module-4-vla/","permalink":"/docs/modules/module-4-vla/","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/modules/module-4-vla/index.md","tags":[],"version":"current","sidebarPosition":4,"frontMatter":{"sidebar_position":4},"sidebar":"tutorialSidebar","previous":{"title":"Nav2 for Bipedal Humanoid Movement","permalink":"/docs/modules/module-3-ai-brain/nav2-bipedal"},"next":{"title":"Voice-to-Action (Whisper)","permalink":"/docs/modules/module-4-vla/whisper-voice-action"}}');var s=i(4848),t=i(8453);const a={sidebar_position:4},l="Module 4: Vision-Language-Action (VLA)",r={},d=[{value:"Overview",id:"overview",level:2},{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Chapters",id:"chapters",level:2},{value:"Prerequisites",id:"prerequisites",level:2},{value:"RAG Summary",id:"rag-summary",level:2},{value:"Next Steps",id:"next-steps",level:2}];function c(e){const n={a:"a",h1:"h1",h2:"h2",header:"header",li:"li",ol:"ol",p:"p",ul:"ul",...(0,t.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"module-4-vision-language-action-vla",children:"Module 4: Vision-Language-Action (VLA)"})}),"\n",(0,s.jsx)(n.p,{children:"Welcome to Module 4 of the Physical AI & Humanoid Robotics book. This module focuses on Vision-Language-Action systems that enable humanoid robots to understand and respond to human commands through voice and visual input."}),"\n",(0,s.jsx)(n.h2,{id:"overview",children:"Overview"}),"\n",(0,s.jsx)(n.p,{children:"This module covers:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Voice-to-action systems using Whisper"}),"\n",(0,s.jsx)(n.li,{children:"Cognitive planning with LLMs for ROS 2 actions"}),"\n",(0,s.jsx)(n.li,{children:"Capstone autonomous humanoid pipeline"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,s.jsx)(n.p,{children:"By the end of this module, you will:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Understand voice recognition and command processing with Whisper"}),"\n",(0,s.jsx)(n.li,{children:"Know how to integrate LLMs for cognitive planning"}),"\n",(0,s.jsx)(n.li,{children:"Learn to connect voice commands to ROS 2 actions"}),"\n",(0,s.jsx)(n.li,{children:"Build an autonomous humanoid pipeline"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"chapters",children:"Chapters"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"/docs/modules/module-4-vla/whisper-voice-action",children:"Voice-to-Action (Whisper)"})," - Voice recognition and command processing"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"/docs/modules/module-4-vla/llm-cognitive-planning",children:"Cognitive Planning (LLMs \u2192 ROS 2 actions)"})," - Planning with Large Language Models"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"/docs/modules/module-4-vla/autonomous-humanoid",children:"Capstone: Autonomous Humanoid Pipeline"})," - Complete integrated system"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,s.jsx)(n.p,{children:"Before starting this module, ensure you have:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Understanding of speech recognition concepts"}),"\n",(0,s.jsx)(n.li,{children:"Familiarity with Large Language Models"}),"\n",(0,s.jsx)(n.li,{children:"Knowledge of ROS 2 action systems"}),"\n",(0,s.jsx)(n.li,{children:"Experience with natural language processing"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"rag-summary",children:"RAG Summary"}),"\n",(0,s.jsx)(n.p,{children:"Module 4 covers Vision-Language-Action systems that enable humanoid robots to process voice commands using Whisper, perform cognitive planning with LLMs, and execute actions through ROS 2. These technologies create an integrated pipeline for human-robot interaction and autonomous behavior."}),"\n",(0,s.jsx)(n.h2,{id:"next-steps",children:"Next Steps"}),"\n",(0,s.jsx)(n.p,{children:"After completing this module, you will have learned the complete pipeline from voice commands to humanoid robot actions, completing the end-to-end robotics system."})]})}function u(e={}){const{wrapper:n}={...(0,t.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(c,{...e})}):c(e)}},8453:(e,n,i)=>{i.d(n,{R:()=>a});var o=i(6540);const s={},t=o.createContext(s);function a(e){const n=o.useContext(t);return o.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}}}]);