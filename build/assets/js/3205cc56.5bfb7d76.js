"use strict";(globalThis.webpackChunkphysical_ai_humanoid_robotics=globalThis.webpackChunkphysical_ai_humanoid_robotics||[]).push([[590],{8308:(n,e,t)=>{t.r(e),t.d(e,{assets:()=>r,contentTitle:()=>l,default:()=>g,frontMatter:()=>s,metadata:()=>a,toc:()=>c});const a=JSON.parse('{"id":"modules/module-4-vla/llm-cognitive-planning","title":"Cognitive Planning (LLMs \u2192 ROS 2 actions)","description":"Introduction","source":"@site/docs/modules/module-4-vla/02-llm-cognitive-planning.mdx","sourceDirName":"modules/module-4-vla","slug":"/modules/module-4-vla/llm-cognitive-planning","permalink":"/docs/modules/module-4-vla/llm-cognitive-planning","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/modules/module-4-vla/02-llm-cognitive-planning.mdx","tags":[],"version":"current","sidebarPosition":2,"frontMatter":{"sidebar_label":"Cognitive Planning (LLMs \u2192 ROS 2 actions)","sidebar_position":2},"sidebar":"tutorialSidebar","previous":{"title":"Voice-to-Action (Whisper)","permalink":"/docs/modules/module-4-vla/whisper-voice-action"},"next":{"title":"Capstone: Autonomous Humanoid Pipeline","permalink":"/docs/modules/module-4-vla/autonomous-humanoid"}}');var i=t(4848),o=t(8453);const s={sidebar_label:"Cognitive Planning (LLMs \u2192 ROS 2 actions)",sidebar_position:2},l="Cognitive Planning (LLMs \u2192 ROS 2 actions)",r={},c=[{value:"Introduction",id:"introduction",level:2},{value:"LLM Integration Architecture",id:"llm-integration-architecture",level:2},{value:"Cognitive Planning Pipeline",id:"cognitive-planning-pipeline",level:3},{value:"Core Components",id:"core-components",level:3},{value:"Context-Aware Planning",id:"context-aware-planning",level:2},{value:"Maintaining World State",id:"maintaining-world-state",level:3},{value:"Multi-Modal Integration",id:"multi-modal-integration",level:2},{value:"Combining LLM with Perception",id:"combining-llm-with-perception",level:3},{value:"Task and Motion Planning Integration",id:"task-and-motion-planning-integration",level:2},{value:"Combining High-Level Planning with Low-Level Control",id:"combining-high-level-planning-with-low-level-control",level:3},{value:"Error Handling and Recovery",id:"error-handling-and-recovery",level:2},{value:"Robust Planning with Error Recovery",id:"robust-planning-with-error-recovery",level:3},{value:"Performance Optimization",id:"performance-optimization",level:2},{value:"Efficient LLM Integration",id:"efficient-llm-integration",level:3},{value:"Integration with ROS 2 Ecosystem",id:"integration-with-ros-2-ecosystem",level:2},{value:"Complete Integration Example",id:"complete-integration-example",level:3},{value:"Troubleshooting Common Issues",id:"troubleshooting-common-issues",level:2},{value:"Common LLM Integration Problems",id:"common-llm-integration-problems",level:3},{value:"RAG Summary",id:"rag-summary",level:2},{value:"Knowledge Check",id:"knowledge-check",level:2}];function p(n){const e={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",...(0,o.R)(),...n.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(e.header,{children:(0,i.jsx)(e.h1,{id:"cognitive-planning-llms--ros-2-actions",children:"Cognitive Planning (LLMs \u2192 ROS 2 actions)"})}),"\n",(0,i.jsx)(e.h2,{id:"introduction",children:"Introduction"}),"\n",(0,i.jsx)(e.p,{children:"Large Language Models (LLMs) enable sophisticated cognitive planning for humanoid robots by translating high-level goals and natural language commands into executable ROS 2 action sequences. This chapter explores how to integrate LLMs like GPT, Claude, or open-source alternatives into robotic systems for intelligent task planning and execution."}),"\n",(0,i.jsx)(e.h2,{id:"llm-integration-architecture",children:"LLM Integration Architecture"}),"\n",(0,i.jsx)(e.h3,{id:"cognitive-planning-pipeline",children:"Cognitive Planning Pipeline"}),"\n",(0,i.jsx)(e.p,{children:"The cognitive planning system creates a pipeline from high-level goals to low-level actions:"}),"\n",(0,i.jsxs)(e.ol,{children:["\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Goal Input"}),": Natural language goals or commands"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Semantic Understanding"}),": LLM interpretation of goals"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Task Decomposition"}),": Breaking goals into subtasks"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Action Mapping"}),": Converting subtasks to ROS 2 actions"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Execution Planning"}),": Sequencing and scheduling actions"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Feedback Integration"}),": Updating plan based on execution results"]}),"\n"]}),"\n",(0,i.jsx)(e.h3,{id:"core-components",children:"Core Components"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:"# Example cognitive planning system architecture\nimport rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nfrom action_msgs.msg import GoalStatus\nfrom rclpy.action import ActionClient\nfrom rclpy.callback_groups import ReentrantCallbackGroup\nimport openai\nimport json\nimport asyncio\nfrom typing import Dict, List, Any, Optional\n\nclass CognitivePlanningNode(Node):\n    def __init__(self):\n        super().__init__('cognitive_planning_node')\n\n        # LLM configuration\n        self.declare_parameter('llm_model', 'gpt-3.5-turbo')\n        self.declare_parameter('openai_api_key', '')\n        self.declare_parameter('max_tokens', 1000)\n\n        self.llm_model = self.get_parameter('llm_model').value\n        self.openai_api_key = self.get_parameter('openai_api_key').value\n        self.max_tokens = self.get_parameter('max_tokens').value\n\n        # Initialize OpenAI client\n        openai.api_key = self.openai_api_key\n        self.client = openai.OpenAI(api_key=self.openai_api_key)\n\n        # Publishers and subscriptions\n        self.goal_subscription = self.create_subscription(\n            String,\n            'high_level_goal',\n            self.goal_callback,\n            10\n        )\n\n        self.plan_publisher = self.create_publisher(\n            String,\n            'action_plan',\n            10\n        )\n\n        self.status_publisher = self.create_publisher(\n            String,\n            'planning_status',\n            10\n        )\n\n        # Action clients for different robot capabilities\n        self.action_clients = {\n            'navigation': ActionClient(self, 'nav2_msgs.action.NavigateToPose', 'navigate_to_pose'),\n            'arm_manipulation': ActionClient(self, 'control_msgs.action.FollowJointTrajectory', 'arm_controller/follow_joint_trajectory'),\n            'gripper': ActionClient(self, 'control_msgs.action.GripperCommand', 'gripper_controller/gripper_cmd'),\n            'speech': ActionClient(self, 'tts_msgs.action.SynthesizeSpeech', 'text_to_speech/synthesize_speech')\n        }\n\n        # Robot state and capabilities\n        self.robot_capabilities = self.initialize_robot_capabilities()\n        self.current_plan = None\n        self.plan_execution_active = False\n\n    def initialize_robot_capabilities(self) -> Dict[str, Any]:\n        \"\"\"Initialize robot capabilities for LLM planning\"\"\"\n        return {\n            'navigation': {\n                'supported': True,\n                'range': 'unlimited',\n                'precision': '0.1m',\n                'actions': ['move_to', 'go_to', 'navigate_to', 'approach']\n            },\n            'manipulation': {\n                'supported': True,\n                'reach': '1.0m',\n                'payload': '5kg',\n                'actions': ['pick', 'place', 'grasp', 'release', 'move_object']\n            },\n            'perception': {\n                'supported': True,\n                'range': '10m',\n                'actions': ['detect', 'identify', 'recognize', 'find', 'locate']\n            },\n            'communication': {\n                'supported': True,\n                'actions': ['speak', 'say', 'tell', 'announce']\n            }\n        }\n\n    async def plan_with_llm(self, goal: str) -> Optional[List[Dict[str, Any]]]:\n        \"\"\"Generate action plan using LLM\"\"\"\n        system_prompt = f\"\"\"\n        You are a cognitive planning assistant for a humanoid robot. Your role is to decompose high-level goals into executable action sequences.\n\n        Robot capabilities:\n        {json.dumps(self.robot_capabilities, indent=2)}\n\n        Available ROS 2 action types:\n        - navigation: move_to(location), go_to(x, y, theta)\n        - manipulation: pick(object), place(object, location), grasp(object), release()\n        - perception: detect(object), identify(object), recognize(object), find(object), locate(object)\n        - communication: speak(text)\n\n        Output format: Return a JSON list of actions, each with:\n        {{\n            \"action_type\": \"navigation|manipulation|perception|communication\",\n            \"action_name\": \"specific action\",\n            \"parameters\": {{\"param1\": \"value1\", ...}},\n            \"description\": \"Human-readable description\"\n        }}\n\n        Rules:\n        1. Only use actions that match robot capabilities\n        2. Ensure action sequence is logical and executable\n        3. Include necessary perception actions before manipulation\n        4. Add communication actions for user feedback when appropriate\n        \"\"\"\n\n        user_prompt = f\"Goal: {goal}\\n\\nGenerate a detailed action plan:\"\n\n        try:\n            response = self.client.chat.completions.create(\n                model=self.llm_model,\n                messages=[\n                    {\"role\": \"system\", \"content\": system_prompt},\n                    {\"role\": \"user\", \"content\": user_prompt}\n                ],\n                max_tokens=self.max_tokens,\n                temperature=0.3,\n                response_format={\"type\": \"json_object\"}\n            )\n\n            # Extract and parse the plan\n            plan_text = response.choices[0].message.content\n            plan_json = json.loads(plan_text)\n\n            if 'actions' in plan_json:\n                return plan_json['actions']\n            else:\n                return plan_json  # If the response is already a list\n\n        except Exception as e:\n            self.get_logger().error(f'LLM planning error: {e}')\n            return None\n\n    def goal_callback(self, msg: String):\n        \"\"\"Process high-level goal\"\"\"\n        goal = msg.data\n        self.get_logger().info(f'Received high-level goal: {goal}')\n\n        # Publish planning status\n        status_msg = String()\n        status_msg.data = f'Planning for goal: {goal}'\n        self.status_publisher.publish(status_msg)\n\n        # Plan asynchronously\n        asyncio.create_task(self.execute_planning_workflow(goal))\n\n    async def execute_planning_workflow(self, goal: str):\n        \"\"\"Execute the complete planning and execution workflow\"\"\"\n        # Generate plan with LLM\n        plan = await self.plan_with_llm(goal)\n\n        if not plan:\n            self.get_logger().error('Failed to generate plan')\n            return\n\n        # Publish the plan\n        plan_msg = String()\n        plan_msg.data = json.dumps(plan, indent=2)\n        self.plan_publisher.publish(plan_msg)\n\n        self.get_logger().info(f'Generated plan with {len(plan)} actions')\n\n        # Execute the plan\n        await self.execute_plan(plan)\n\n    async def execute_plan(self, plan: List[Dict[str, Any]]):\n        \"\"\"Execute the action plan\"\"\"\n        self.plan_execution_active = True\n        self.current_plan = plan\n\n        for i, action in enumerate(plan):\n            if not self.plan_execution_active:\n                self.get_logger().info('Plan execution cancelled')\n                break\n\n            self.get_logger().info(f'Executing action {i+1}/{len(plan)}: {action[\"action_name\"]}')\n\n            success = await self.execute_single_action(action)\n\n            if not success:\n                self.get_logger().error(f'Action failed: {action[\"action_name\"]}')\n                # Implement recovery strategies here\n                break\n\n        self.plan_execution_active = False\n        self.current_plan = None\n\n    async def execute_single_action(self, action: Dict[str, Any]) -> bool:\n        \"\"\"Execute a single action\"\"\"\n        action_type = action['action_type']\n        action_name = action['action_name']\n        parameters = action.get('parameters', {})\n\n        if action_type == 'navigation':\n            return await self.execute_navigation_action(action_name, parameters)\n        elif action_type == 'manipulation':\n            return await self.execute_manipulation_action(action_name, parameters)\n        elif action_type == 'perception':\n            return await self.execute_perception_action(action_name, parameters)\n        elif action_type == 'communication':\n            return await self.execute_communication_action(action_name, parameters)\n        else:\n            self.get_logger().error(f'Unknown action type: {action_type}')\n            return False\n\n    async def execute_navigation_action(self, action_name: str, parameters: Dict[str, Any]) -> bool:\n        \"\"\"Execute navigation action\"\"\"\n        # Example: move_to(location) or go_to(x, y, theta)\n        if action_name == 'move_to' or action_name == 'go_to':\n            x = parameters.get('x', 0.0)\n            y = parameters.get('y', 0.0)\n            theta = parameters.get('theta', 0.0)\n\n            # Create navigation goal\n            from geometry_msgs.msg import PoseStamped\n            from nav2_msgs.action import NavigateToPose\n\n            goal_msg = NavigateToPose.Goal()\n            goal_msg.pose.header.frame_id = 'map'\n            goal_msg.pose.pose.position.x = float(x)\n            goal_msg.pose.pose.position.y = float(y)\n            goal_msg.pose.pose.position.z = 0.0\n\n            # Convert theta to quaternion (simplified)\n            from math import sin, cos\n            goal_msg.pose.pose.orientation.z = sin(theta / 2.0)\n            goal_msg.pose.pose.orientation.w = cos(theta / 2.0)\n\n            # Send navigation goal\n            if 'navigation' in self.action_clients:\n                client = self.action_clients['navigation']\n                if client.wait_for_server(timeout_sec=5.0):\n                    future = client.send_goal_async(goal_msg)\n                    result = await future\n                    return result.status == GoalStatus.STATUS_SUCCEEDED\n\n        return False\n"})}),"\n",(0,i.jsx)(e.h2,{id:"context-aware-planning",children:"Context-Aware Planning"}),"\n",(0,i.jsx)(e.h3,{id:"maintaining-world-state",children:"Maintaining World State"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:'# Example of context-aware planning with world state\nimport rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nfrom sensor_msgs.msg import LaserScan, Image\nfrom geometry_msgs.msg import PoseStamped\nfrom typing import Dict, Any, List\nimport json\n\nclass ContextAwarePlanningNode(Node):\n    def __init__(self):\n        super().__init__(\'context_aware_planning\')\n\n        # Initialize world state\n        self.world_state = {\n            \'robot_pose\': {\'x\': 0.0, \'y\': 0.0, \'theta\': 0.0},\n            \'objects\': {},  # object_name: {\'position\': {\'x\': x, \'y\': y}, \'type\': \'type\'}\n            \'locations\': {},  # location_name: {\'x\': x, \'y\': y}\n            \'robot_status\': \'idle\',\n            \'battery_level\': 100.0,\n            \'current_task\': None\n        }\n\n        # Subscriptions for context updates\n        self.pose_subscription = self.create_subscription(\n            PoseStamped,\n            \'robot_pose\',\n            self.pose_callback,\n            10\n        )\n\n        self.scan_subscription = self.create_subscription(\n            LaserScan,\n            \'scan\',\n            self.scan_callback,\n            10\n        )\n\n        self.object_subscription = self.create_subscription(\n            String,\n            \'detected_objects\',\n            self.object_callback,\n            10\n        )\n\n        # Goal processing\n        self.goal_subscription = self.create_subscription(\n            String,\n            \'high_level_goal\',\n            self.contextual_goal_callback,\n            10\n        )\n\n        self.plan_publisher = self.create_publisher(String, \'contextual_action_plan\', 10)\n\n    def pose_callback(self, msg: PoseStamped):\n        """Update robot pose in world state"""\n        self.world_state[\'robot_pose\'] = {\n            \'x\': msg.pose.position.x,\n            \'y\': msg.pose.position.y,\n            \'theta\': self.quaternion_to_yaw(msg.pose.orientation)\n        }\n\n    def scan_callback(self, msg: LaserScan):\n        """Process laser scan for obstacle information"""\n        # Update world state with obstacle information\n        # This is a simplified example\n        pass\n\n    def object_callback(self, msg: String):\n        """Process detected objects"""\n        try:\n            detected_objects = json.loads(msg.data)\n            for obj in detected_objects:\n                self.world_state[\'objects\'][obj[\'name\']] = {\n                    \'position\': obj[\'position\'],\n                    \'type\': obj[\'type\'],\n                    \'confidence\': obj[\'confidence\']\n                }\n        except json.JSONDecodeError:\n            self.get_logger().error(\'Failed to parse detected objects\')\n\n    def contextual_goal_callback(self, msg: String):\n        """Process goal with context awareness"""\n        goal = msg.data\n        self.get_logger().info(f\'Received contextual goal: {goal}\')\n\n        # Generate contextual plan\n        plan = self.generate_contextual_plan(goal)\n\n        if plan:\n            plan_msg = String()\n            plan_msg.data = json.dumps(plan, indent=2)\n            self.plan_publisher.publish(plan_msg)\n\n    def generate_contextual_plan(self, goal: str) -> List[Dict[str, Any]]:\n        """Generate plan considering current context"""\n        # Use LLM with context for planning\n        system_prompt = f"""\n        You are a context-aware planning assistant for a humanoid robot.\n\n        Current world state:\n        {json.dumps(self.world_state, indent=2)}\n\n        Available actions:\n        - navigation: move_to(location), go_to(x, y, theta)\n        - manipulation: pick(object), place(object, location), grasp(object), release()\n        - perception: detect(object), identify(object), recognize(object), find(object), locate(object)\n        - communication: speak(text)\n\n        Generate an action plan that considers the current world state.\n        """\n\n        user_prompt = f"Goal: {goal}\\n\\nGenerate a context-aware action plan:"\n\n        # In a real implementation, this would call the LLM\n        # For this example, we\'ll return a mock plan\n        return [\n            {\n                "action_type": "perception",\n                "action_name": "detect",\n                "parameters": {"object": "target"},\n                "description": "Detect the target object in the environment"\n            },\n            {\n                "action_type": "navigation",\n                "action_name": "go_to",\n                "parameters": {"x": 1.0, "y": 1.0, "theta": 0.0},\n                "description": "Navigate to the target location"\n            }\n        ]\n\n    def quaternion_to_yaw(self, orientation):\n        """Convert quaternion to yaw angle"""\n        import math\n        siny_cosp = 2 * (orientation.w * orientation.z + orientation.x * orientation.y)\n        cosy_cosp = 1 - 2 * (orientation.y * orientation.y + orientation.z * orientation.z)\n        return math.atan2(siny_cosp, cosy_cosp)\n'})}),"\n",(0,i.jsx)(e.h2,{id:"multi-modal-integration",children:"Multi-Modal Integration"}),"\n",(0,i.jsx)(e.h3,{id:"combining-llm-with-perception",children:"Combining LLM with Perception"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:'# Example of multi-modal planning with perception integration\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image, CameraInfo\nfrom std_msgs.msg import String\nfrom cv_bridge import CvBridge\nimport cv2\nimport numpy as np\nimport base64\nfrom io import BytesIO\n\nclass MultiModalPlanningNode(Node):\n    def __init__(self):\n        super().__init__(\'multi_modal_planning\')\n\n        # Initialize CV bridge\n        self.cv_bridge = CvBridge()\n\n        # Subscriptions for multi-modal input\n        self.image_subscription = self.create_subscription(\n            Image,\n            \'camera/image_raw\',\n            self.image_callback,\n            10\n        )\n\n        self.goal_subscription = self.create_subscription(\n            String,\n            \'high_level_goal\',\n            self.multi_modal_goal_callback,\n            10\n        )\n\n        self.plan_publisher = self.create_publisher(String, \'multi_modal_plan\', 10)\n\n        # Store latest image for planning context\n        self.latest_image = None\n        self.latest_image_timestamp = None\n\n    def image_callback(self, msg: Image):\n        """Process incoming image"""\n        try:\n            cv_image = self.cv_bridge.imgmsg_to_cv2(msg, desired_encoding=\'bgr8\')\n            self.latest_image = cv_image\n            self.latest_image_timestamp = msg.header.stamp\n        except Exception as e:\n            self.get_logger().error(f\'Image conversion error: {e}\')\n\n    def multi_modal_goal_callback(self, msg: String):\n        """Process goal with multi-modal context"""\n        goal = msg.data\n        self.get_logger().info(f\'Received multi-modal goal: {goal}\')\n\n        # Encode latest image for LLM context\n        image_b64 = None\n        if self.latest_image is not None:\n            # Convert image to base64 for LLM\n            _, buffer = cv2.imencode(\'.jpg\', self.latest_image)\n            image_b64 = base64.b64encode(buffer).decode(\'utf-8\')\n\n        # Generate plan with both text and visual context\n        plan = self.generate_multi_modal_plan(goal, image_b64)\n\n        if plan:\n            plan_msg = String()\n            plan_msg.data = json.dumps(plan, indent=2)\n            self.plan_publisher.publish(plan_msg)\n\n    def generate_multi_modal_plan(self, goal: str, image_b64: str) -> List[Dict[str, Any]]:\n        """Generate plan using both text and visual context"""\n        if image_b64:\n            # This would use a multi-modal LLM like GPT-4 Vision\n            # For this example, we\'ll simulate the process\n            system_prompt = f"""\n            You are a multi-modal planning assistant. The user has provided both a text goal and a visual context.\n\n            Text goal: {goal}\n\n            Visual context: The image shows the current environment from the robot\'s perspective.\n\n            Generate an action plan considering both the text goal and visual information.\n            """\n\n            # In a real implementation, this would call a multi-modal LLM\n            # For now, return a mock plan\n            return [\n                {\n                    "action_type": "perception",\n                    "action_name": "analyze_image",\n                    "parameters": {"image_context": "visual_analysis_performed"},\n                    "description": "Analyzed the visual context to understand the environment"\n                },\n                {\n                    "action_type": "navigation",\n                    "action_name": "move_to_safe_location",\n                    "parameters": {"x": 2.0, "y": 2.0, "theta": 0.0},\n                    "description": "Moved to a safe location based on visual analysis"\n                }\n            ]\n        else:\n            # Fall back to text-only planning\n            return [\n                {\n                    "action_type": "navigation",\n                    "action_name": "explore",\n                    "parameters": {"area": "unknown"},\n                    "description": "Exploring unknown area to gather visual information"\n                }\n            ]\n'})}),"\n",(0,i.jsx)(e.h2,{id:"task-and-motion-planning-integration",children:"Task and Motion Planning Integration"}),"\n",(0,i.jsx)(e.h3,{id:"combining-high-level-planning-with-low-level-control",children:"Combining High-Level Planning with Low-Level Control"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:'# Example of task and motion planning integration\nimport rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nfrom geometry_msgs.msg import Pose, Point\nfrom moveit_msgs.msg import MoveItErrorCodes\nfrom moveit_msgs.srv import GetMotionPlan\nfrom trajectory_msgs.msg import JointTrajectory\nfrom sensor_msgs.msg import JointState\n\nclass TaskMotionPlanningNode(Node):\n    def __init__(self):\n        super().__init__(\'task_motion_planning\')\n\n        # Initialize MoveIt interface\n        self.moveit_client = self.create_client(\n            GetMotionPlan,\n            \'plan_kinematic_path\'\n        )\n\n        # LLM planning integration\n        self.goal_subscription = self.create_subscription(\n            String,\n            \'high_level_goal\',\n            self.task_motion_goal_callback,\n            10\n        )\n\n        self.trajectory_publisher = self.create_publisher(\n            JointTrajectory,\n            \'joint_trajectory\',\n            10\n        )\n\n        # Robot state\n        self.joint_state_subscription = self.create_subscription(\n            JointState,\n            \'joint_states\',\n            self.joint_state_callback,\n            10\n        )\n\n        self.current_joint_positions = {}\n\n    def joint_state_callback(self, msg: JointState):\n        """Update current joint positions"""\n        for i, name in enumerate(msg.name):\n            self.current_joint_positions[name] = msg.position[i]\n\n    def task_motion_goal_callback(self, msg: String):\n        """Process goal requiring both task and motion planning"""\n        goal = msg.data\n        self.get_logger().info(f\'Received task-motion goal: {goal}\')\n\n        # First, use LLM to decompose the task\n        task_plan = self.decompose_task_with_llm(goal)\n\n        # Then, for each manipulation task, plan the motion\n        for task in task_plan:\n            if task[\'action_type\'] == \'manipulation\':\n                trajectory = self.plan_manipulation_motion(task)\n                if trajectory:\n                    self.execute_trajectory(trajectory)\n\n    def decompose_task_with_llm(self, goal: str) -> List[Dict[str, Any]]:\n        """Decompose high-level goal into subtasks using LLM"""\n        system_prompt = """\n        Decompose the high-level goal into specific subtasks that can be executed by a robot.\n        Each subtask should be specific enough for motion planning.\n\n        Subtask format:\n        {\n            "action_type": "navigation|manipulation|perception",\n            "action_name": "specific action",\n            "parameters": {\n                "target_pose": {"x": x, "y": y, "z": z, "qx": qx, "qy": qy, "qz": qz, "qw": qw},\n                "joint_targets": {"joint_name": position, ...},\n                "object_name": "name"\n            },\n            "description": "What this subtask achieves"\n        }\n        """\n\n        user_prompt = f"Goal: {goal}\\n\\nDecompose into executable subtasks:"\n\n        # In a real implementation, this would call the LLM\n        # For this example, return a mock plan\n        return [\n            {\n                "action_type": "navigation",\n                "action_name": "move_to_object",\n                "parameters": {\n                    "target_pose": {"x": 1.0, "y": 0.5, "z": 0.0, "qx": 0.0, "qy": 0.0, "qz": 0.0, "qw": 1.0}\n                },\n                "description": "Move to position near the object to be manipulated"\n            },\n            {\n                "action_type": "manipulation",\n                "action_name": "grasp_object",\n                "parameters": {\n                    "target_pose": {"x": 1.0, "y": 0.5, "z": 0.8, "qx": 0.0, "qy": 0.0, "qz": 0.0, "qw": 1.0},\n                    "joint_targets": {"gripper_joint": 0.5}\n                },\n                "description": "Grasp the target object"\n            }\n        ]\n\n    def plan_manipulation_motion(self, task: Dict[str, Any]) -> Optional[JointTrajectory]:\n        """Plan motion for manipulation task"""\n        if \'target_pose\' in task[\'parameters\']:\n            target_pose = task[\'parameters\'][\'target_pose\']\n\n            # Create motion planning request\n            # In a real implementation, this would use MoveIt\n            # For this example, return a mock trajectory\n            trajectory = JointTrajectory()\n            trajectory.joint_names = [\'joint1\', \'joint2\', \'joint3\', \'gripper_joint\']\n\n            # Create trajectory points\n            point = JointTrajectoryPoint()\n            point.positions = [0.0, 0.0, 0.0, 0.0]  # Starting position\n            point.time_from_start.sec = 1\n            trajectory.points.append(point)\n\n            # Add target position (this would be calculated by motion planner)\n            target_point = JointTrajectoryPoint()\n            target_point.positions = [1.0, 0.5, 0.3, 0.5]  # Example target\n            target_point.time_from_start.sec = 3\n            trajectory.points.append(target_point)\n\n            return trajectory\n\n        return None\n\n    def execute_trajectory(self, trajectory: JointTrajectory):\n        """Execute the planned trajectory"""\n        self.trajectory_publisher.publish(trajectory)\n        self.get_logger().info(\'Published trajectory for execution\')\n'})}),"\n",(0,i.jsx)(e.h2,{id:"error-handling-and-recovery",children:"Error Handling and Recovery"}),"\n",(0,i.jsx)(e.h3,{id:"robust-planning-with-error-recovery",children:"Robust Planning with Error Recovery"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:'# Example of error handling and recovery in LLM-based planning\nimport rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nfrom action_msgs.msg import GoalStatus\nfrom rclpy.action import ActionClient\nfrom typing import Dict, List, Any, Optional\nimport random\n\nclass RobustPlanningNode(Node):\n    def __init__(self):\n        super().__init__(\'robust_planning\')\n\n        # Initialize with error handling\n        self.goal_subscription = self.create_subscription(\n            String,\n            \'high_level_goal\',\n            self.robust_goal_callback,\n            10\n        )\n\n        self.status_publisher = self.create_publisher(String, \'planning_status\', 10)\n        self.recovery_publisher = self.create_publisher(String, \'recovery_actions\', 10)\n\n        # Action clients with error handling\n        self.action_clients = {}\n\n        # Recovery strategies\n        self.recovery_strategies = {\n            \'navigation_failure\': [\n                \'try_alternative_path\',\n                \'request_human_assistance\',\n                \'return_to_known_location\'\n            ],\n            \'manipulation_failure\': [\n                \'reposition_robot\',\n                \'retry_with_different_approach\',\n                \'request_human_intervention\'\n            ],\n            \'perception_failure\': [\n                \'change_sensor_mode\',\n                \'move_to_better_viewpoint\',\n                \'request_human_verification\'\n            ]\n        }\n\n    def robust_goal_callback(self, msg: String):\n        """Process goal with robust error handling"""\n        goal = msg.data\n        self.get_logger().info(f\'Received goal for robust planning: {goal}\')\n\n        # Generate plan with error handling considerations\n        plan = self.generate_robust_plan(goal)\n\n        if plan:\n            # Execute plan with error monitoring\n            self.execute_plan_with_monitoring(plan)\n\n    def generate_robust_plan(self, goal: str) -> Optional[List[Dict[str, Any]]]:\n        """Generate plan with built-in error handling"""\n        # Use LLM to generate plan considering potential failure points\n        system_prompt = f"""\n        Generate a robust action plan that includes potential failure points and recovery strategies.\n\n        For each action, consider:\n        1. Potential failure modes\n        2. Conditions that might cause failure\n        3. Recovery strategies for each potential failure\n\n        Output format:\n        {{\n            "actions": [\n                {{\n                    "action_type": "type",\n                    "action_name": "name",\n                    "parameters": {{...}},\n                    "description": "description",\n                    "failure_modes": ["mode1", "mode2", ...],\n                    "recovery_strategies": ["strategy1", "strategy2", ...]\n                }}\n            ]\n        }}\n        """\n\n        user_prompt = f"Goal: {goal}\\n\\nGenerate robust action plan:"\n\n        # In a real implementation, this would call the LLM\n        # For this example, return a mock robust plan\n        return [\n            {\n                "action_type": "navigation",\n                "action_name": "move_to_kitchen",\n                "parameters": {"x": 5.0, "y": 3.0, "theta": 0.0},\n                "description": "Move to kitchen to fetch item",\n                "failure_modes": ["obstacle_detected", "localization_failure"],\n                "recovery_strategies": ["use_alternative_path", "relocalize"]\n            },\n            {\n                "action_type": "manipulation",\n                "action_name": "pick_up_cup",\n                "parameters": {"object": "cup", "approach": "top_down"},\n                "description": "Pick up the cup",\n                "failure_modes": ["grasp_failure", "object_not_found"],\n                "recovery_strategies": ["adjust_grasp", "search_alternative_objects"]\n            }\n        ]\n\n    def execute_plan_with_monitoring(self, plan: List[Dict[str, Any]]):\n        """Execute plan with monitoring and recovery"""\n        for i, action in enumerate(plan):\n            self.get_logger().info(f\'Executing action {i+1}/{len(plan)}: {action["action_name"]}\')\n\n            # Execute action and monitor for success/failure\n            success = self.execute_monitored_action(action)\n\n            if not success:\n                self.get_logger().warn(f\'Action failed: {action["action_name"]}\')\n\n                # Try recovery strategies\n                recovery_success = self.attempt_recovery(action)\n\n                if not recovery_success:\n                    self.get_logger().error(f\'All recovery strategies failed for action: {action["action_name"]}\')\n                    self.publish_status(f\'Planning failed after recovery attempts for: {action["description"]}\')\n                    return\n                else:\n                    self.get_logger().info(f\'Recovery successful for action: {action["action_name"]}\')\n\n        self.get_logger().info(\'Plan completed successfully\')\n\n    def execute_monitored_action(self, action: Dict[str, Any]) -> bool:\n        """Execute action with monitoring"""\n        # Simulate action execution with potential for failure\n        # In a real implementation, this would execute the actual action\n        # and monitor its progress\n\n        # Simulate success/failure (in real system, check actual execution)\n        success_probability = 0.8  # 80% success rate for this example\n        return random.random() < success_probability\n\n    def attempt_recovery(self, failed_action: Dict[str, Any]) -> bool:\n        """Attempt recovery strategies for failed action"""\n        recovery_strategies = failed_action.get(\'recovery_strategies\', [])\n\n        if not recovery_strategies:\n            self.get_logger().warn(\'No recovery strategies available for failed action\')\n            return False\n\n        for strategy in recovery_strategies:\n            self.get_logger().info(f\'Trying recovery strategy: {strategy}\')\n\n            # Execute recovery strategy\n            strategy_success = self.execute_recovery_strategy(strategy, failed_action)\n\n            if strategy_success:\n                self.get_logger().info(f\'Recovery strategy successful: {strategy}\')\n                recovery_msg = String()\n                recovery_msg.data = f\'Successfully recovered using strategy: {strategy}\'\n                self.recovery_publisher.publish(recovery_msg)\n                return True\n            else:\n                self.get_logger().warn(f\'Recovery strategy failed: {strategy}\')\n\n        return False\n\n    def execute_recovery_strategy(self, strategy: str, failed_action: Dict[str, Any]) -> bool:\n        """Execute a specific recovery strategy"""\n        # Implement specific recovery strategies\n        if strategy == \'use_alternative_path\':\n            # Implement alternative path navigation\n            return self.execute_alternative_navigation(failed_action)\n        elif strategy == \'relocalize\':\n            # Implement relocalization\n            return self.execute_relocalization()\n        elif strategy == \'adjust_grasp\':\n            # Implement grasp adjustment\n            return self.execute_grasp_adjustment(failed_action)\n        elif strategy == \'search_alternative_objects\':\n            # Implement search for alternative objects\n            return self.execute_alternative_object_search(failed_action)\n        else:\n            self.get_logger().warn(f\'Unknown recovery strategy: {strategy}\')\n            return False\n\n    def execute_alternative_navigation(self, failed_action: Dict[str, Any]) -> bool:\n        """Execute alternative navigation strategy"""\n        # This would implement alternative path planning\n        # For this example, simulate success\n        self.get_logger().info(\'Executing alternative navigation\')\n        return True\n\n    def execute_relocalization(self) -> bool:\n        """Execute relocalization strategy"""\n        # This would trigger robot relocalization\n        self.get_logger().info(\'Executing relocalization\')\n        return True\n\n    def execute_grasp_adjustment(self, failed_action: Dict[str, Any]) -> bool:\n        """Execute grasp adjustment strategy"""\n        # This would adjust grasp parameters and retry\n        self.get_logger().info(\'Executing grasp adjustment\')\n        return True\n\n    def execute_alternative_object_search(self, failed_action: Dict[str, Any]) -> bool:\n        """Execute search for alternative objects"""\n        # This would search for alternative objects to the target\n        self.get_logger().info(\'Executing alternative object search\')\n        return True\n\n    def publish_status(self, status: str):\n        """Publish planning status"""\n        status_msg = String()\n        status_msg.data = status\n        self.status_publisher.publish(status_msg)\n'})}),"\n",(0,i.jsx)(e.h2,{id:"performance-optimization",children:"Performance Optimization"}),"\n",(0,i.jsx)(e.h3,{id:"efficient-llm-integration",children:"Efficient LLM Integration"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:'# Example of performance optimization techniques\nimport rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nfrom functools import lru_cache\nimport threading\nimport queue\nfrom typing import Optional\n\nclass OptimizedLLMPlanningNode(Node):\n    def __init__(self):\n        super().__init__(\'optimized_llm_planning\')\n\n        # Initialize with optimization considerations\n        self.declare_parameter(\'llm_cache_size\', 100)\n        self.declare_parameter(\'max_concurrent_requests\', 3)\n        self.declare_parameter(\'plan_timeout\', 30.0)\n\n        self.cache_size = self.get_parameter(\'llm_cache_size\').value\n        self.max_concurrent = self.get_parameter(\'max_concurrent_requests\').value\n        self.plan_timeout = self.get_parameter(\'plan_timeout\').value\n\n        # Caching for frequently requested plans\n        self.plan_cache = {}\n\n        # Request queue for managing LLM calls\n        self.request_queue = queue.Queue(maxsize=10)\n        self.response_queue = queue.Queue()\n\n        # Start LLM processing thread\n        self.llm_thread = threading.Thread(target=self.llm_processing_loop)\n        self.llm_thread.daemon = True\n        self.llm_thread.start()\n\n        # Goal processing\n        self.goal_subscription = self.create_subscription(\n            String,\n            \'high_level_goal\',\n            self.optimized_goal_callback,\n            10\n        )\n\n        self.plan_publisher = self.create_publisher(String, \'optimized_action_plan\', 10)\n\n    @lru_cache(maxsize=100)\n    def get_cached_plan(self, goal: str, context: str = "") -> Optional[str]:\n        """Get cached plan if available"""\n        # This is a simplified cache; in practice, you\'d hash the inputs\n        cache_key = f"{goal}_{context}"\n        return self.plan_cache.get(cache_key)\n\n    def set_cached_plan(self, goal: str, context: str, plan: str):\n        """Cache a plan"""\n        cache_key = f"{goal}_{context}"\n        if len(self.plan_cache) >= self.cache_size:\n            # Remove oldest entries\n            oldest_key = next(iter(self.plan_cache))\n            del self.plan_cache[oldest_key]\n        self.plan_cache[cache_key] = plan\n\n    def optimized_goal_callback(self, msg: String):\n        """Process goal with optimization"""\n        goal = msg.data\n\n        # Check cache first\n        cached_plan = self.get_cached_plan(goal)\n        if cached_plan:\n            self.get_logger().info(\'Using cached plan\')\n            plan_msg = String()\n            plan_msg.data = cached_plan\n            self.plan_publisher.publish(plan_msg)\n            return\n\n        # Add to processing queue\n        try:\n            self.request_queue.put_nowait({\n                \'goal\': goal,\n                \'timestamp\': self.get_clock().now()\n            })\n            self.get_logger().info(f\'Added goal to processing queue: {goal}\')\n        except queue.Full:\n            self.get_logger().error(\'Request queue is full, dropping request\')\n\n    def llm_processing_loop(self):\n        """Process LLM requests in a separate thread"""\n        while rclpy.ok():\n            try:\n                # Get request from queue\n                request = self.request_queue.get(timeout=1.0)\n\n                # Process with LLM\n                plan = self.process_with_llm(request[\'goal\'])\n\n                if plan:\n                    # Cache the plan\n                    self.set_cached_plan(request[\'goal\'], "", plan)\n\n                    # Publish result\n                    plan_msg = String()\n                    plan_msg.data = plan\n\n                    # Use call to publish from the processing thread\n                    # In ROS2, this would need to be handled differently\n                    # This is a simplified example\n\n            except queue.Empty:\n                continue  # Continue loop\n            except Exception as e:\n                self.get_logger().error(f\'LLM processing error: {e}\')\n                continue\n\n    def process_with_llm(self, goal: str) -> Optional[str]:\n        """Process goal with LLM"""\n        # This would call the actual LLM\n        # For this example, return a mock plan\n        import json\n        mock_plan = [\n            {\n                "action_type": "navigation",\n                "action_name": "go_to",\n                "parameters": {"x": 1.0, "y": 1.0, "theta": 0.0},\n                "description": "Move to specified location"\n            }\n        ]\n        return json.dumps(mock_plan)\n\n# Performance optimization summary\n"""\nLLM Performance Optimization for Robotics:\n\n1. Caching:\n   - Cache frequently requested plans\n   - Use LRU cache for memory management\n   - Consider semantic similarity for caching\n\n2. Request Management:\n   - Use queues to manage request flow\n   - Limit concurrent requests\n   - Implement timeouts\n\n3. Model Optimization:\n   - Use smaller models for simple tasks\n   - Implement model quantization\n   - Consider edge deployment\n\n4. Context Management:\n   - Minimize context size\n   - Use relevant context only\n   - Implement context window management\n"""\n'})}),"\n",(0,i.jsx)(e.h2,{id:"integration-with-ros-2-ecosystem",children:"Integration with ROS 2 Ecosystem"}),"\n",(0,i.jsx)(e.h3,{id:"complete-integration-example",children:"Complete Integration Example"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:'# Complete integration example combining all concepts\nimport rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nfrom geometry_msgs.msg import Twist, PoseStamped\nfrom sensor_msgs.msg import LaserScan, Image\nfrom action_msgs.msg import GoalStatus\nfrom rclpy.action import ActionClient\nfrom rclpy.callback_groups import MutuallyExclusiveCallbackGroup\nfrom threading import Lock\nimport asyncio\nimport json\nfrom typing import Dict, List, Any, Optional\n\nclass CompleteCognitivePlanningNode(Node):\n    def __init__(self):\n        super().__init__(\'complete_cognitive_planning\')\n\n        # Threading lock for safe access\n        self.lock = Lock()\n\n        # Initialize all required interfaces\n        self.initialize_subscriptions()\n        self.initialize_publishers()\n        self.initialize_action_clients()\n        self.initialize_state()\n\n        # Start async processing\n        self.processing_tasks = []\n\n    def initialize_subscriptions(self):\n        """Initialize all subscriptions"""\n        # Goal input\n        self.goal_subscription = self.create_subscription(\n            String,\n            \'high_level_goal\',\n            self.complete_goal_callback,\n            10\n        )\n\n        # Sensor inputs for context\n        self.laser_subscription = self.create_subscription(\n            LaserScan,\n            \'scan\',\n            self.laser_callback,\n            10\n        )\n\n        self.image_subscription = self.create_subscription(\n            Image,\n            \'camera/image_raw\',\n            self.image_callback,\n            10\n        )\n\n        self.pose_subscription = self.create_subscription(\n            PoseStamped,\n            \'robot_pose\',\n            self.pose_callback,\n            10\n        )\n\n    def initialize_publishers(self):\n        """Initialize all publishers"""\n        self.plan_publisher = self.create_publisher(String, \'action_plan\', 10)\n        self.cmd_vel_publisher = self.create_publisher(Twist, \'cmd_vel\', 10)\n        self.status_publisher = self.create_publisher(String, \'planning_status\', 10)\n\n    def initialize_action_clients(self):\n        """Initialize action clients"""\n        self.action_clients = {}\n\n    def initialize_state(self):\n        """Initialize robot state"""\n        self.world_state = {\n            \'robot_pose\': {\'x\': 0.0, \'y\': 0.0, \'theta\': 0.0},\n            \'battery_level\': 100.0,\n            \'current_plan\': None,\n            \'plan_active\': False\n        }\n\n    def complete_goal_callback(self, msg: String):\n        """Complete goal processing pipeline"""\n        goal = msg.data\n        self.get_logger().info(f\'Received goal: {goal}\')\n\n        # Process in async task to avoid blocking\n        task = asyncio.create_task(self.process_complete_goal(goal))\n        self.processing_tasks.append(task)\n\n    async def process_complete_goal(self, goal: str):\n        """Complete processing pipeline"""\n        with self.lock:\n            # Update status\n            self.publish_status(f\'Processing goal: {goal}\')\n\n            # Generate plan with context\n            plan = await self.generate_contextual_plan(goal)\n\n            if plan:\n                # Publish plan\n                self.publish_plan(plan)\n\n                # Execute plan\n                success = await self.execute_contextual_plan(plan)\n\n                if success:\n                    self.publish_status(\'Plan completed successfully\')\n                else:\n                    self.publish_status(\'Plan execution failed\')\n            else:\n                self.publish_status(\'Failed to generate plan\')\n\n    async def generate_contextual_plan(self, goal: str) -> Optional[List[Dict[str, Any]]]:\n        """Generate plan with full context"""\n        # This would integrate all the concepts from above examples\n        # LLM call with context, multi-modal input, etc.\n        # For this example, return a mock plan\n        return [\n            {\n                "action_type": "navigation",\n                "action_name": "go_to",\n                "parameters": {"x": 1.0, "y": 1.0, "theta": 0.0},\n                "description": "Move to target location"\n            }\n        ]\n\n    async def execute_contextual_plan(self, plan: List[Dict[str, Any]]) -> bool:\n        """Execute plan with context awareness"""\n        self.world_state[\'plan_active\'] = True\n        self.world_state[\'current_plan\'] = plan\n\n        for action in plan:\n            if not self.world_state[\'plan_active\']:\n                break\n\n            success = await self.execute_contextual_action(action)\n            if not success:\n                self.world_state[\'plan_active\'] = False\n                return False\n\n        self.world_state[\'plan_active\'] = False\n        self.world_state[\'current_plan\'] = None\n        return True\n\n    async def execute_contextual_action(self, action: Dict[str, Any]) -> bool:\n        """Execute action with context"""\n        # This would handle each action type with context awareness\n        # For this example, return success\n        return True\n\n    def publish_plan(self, plan: List[Dict[str, Any]]):\n        """Publish generated plan"""\n        plan_msg = String()\n        plan_msg.data = json.dumps(plan, indent=2)\n        self.plan_publisher.publish(plan_msg)\n\n    def publish_status(self, status: str):\n        """Publish planning status"""\n        status_msg = String()\n        status_msg.data = status\n        self.status_publisher.publish(status_msg)\n\n    def laser_callback(self, msg: LaserScan):\n        """Update world state with laser data"""\n        with self.lock:\n            # Process laser data for context\n            pass\n\n    def image_callback(self, msg: Image):\n        """Update world state with image data"""\n        with self.lock:\n            # Process image data for context\n            pass\n\n    def pose_callback(self, msg: PoseStamped):\n        """Update robot pose"""\n        with self.lock:\n            self.world_state[\'robot_pose\'] = {\n                \'x\': msg.pose.position.x,\n                \'y\': msg.pose.position.y,\n                \'theta\': self.quaternion_to_yaw(msg.pose.orientation)\n            }\n\n    def quaternion_to_yaw(self, orientation):\n        """Convert quaternion to yaw"""\n        import math\n        siny_cosp = 2 * (orientation.w * orientation.z + orientation.x * orientation.y)\n        cosy_cosp = 1 - 2 * (orientation.y * orientation.y + orientation.z * orientation.z)\n        return math.atan2(siny_cosp, cosy_cosp)\n\ndef main(args=None):\n    rclpy.init(args=args)\n\n    node = CompleteCognitivePlanningNode()\n\n    try:\n        rclpy.spin(node)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,i.jsx)(e.h2,{id:"troubleshooting-common-issues",children:"Troubleshooting Common Issues"}),"\n",(0,i.jsx)(e.h3,{id:"common-llm-integration-problems",children:"Common LLM Integration Problems"}),"\n",(0,i.jsxs)(e.ol,{children:["\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"API Rate Limits"}),": Implement request queuing and retry logic"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Context Window Overflow"}),": Manage context size and relevance"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Action Mapping Failures"}),": Implement fallback strategies"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Timing Issues"}),": Use async processing for non-blocking operations"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Error Propagation"}),": Implement proper error handling and recovery"]}),"\n"]}),"\n",(0,i.jsx)(e.h2,{id:"rag-summary",children:"RAG Summary"}),"\n",(0,i.jsx)(e.p,{children:"LLM-based cognitive planning translates high-level goals into executable ROS 2 actions through a pipeline of semantic understanding, task decomposition, and action mapping. The system integrates context awareness, multi-modal inputs, and error recovery to create robust planning capabilities for humanoid robots. Proper optimization techniques ensure real-time performance while maintaining planning quality."}),"\n",(0,i.jsx)(e.h2,{id:"knowledge-check",children:"Knowledge Check"}),"\n",(0,i.jsxs)(e.ol,{children:["\n",(0,i.jsx)(e.li,{children:"What are the components of the cognitive planning pipeline?"}),"\n",(0,i.jsx)(e.li,{children:"How do you maintain world state for context-aware planning?"}),"\n",(0,i.jsx)(e.li,{children:"What is multi-modal integration and why is it important?"}),"\n",(0,i.jsx)(e.li,{children:"How do you handle errors and recovery in LLM-based planning?"}),"\n",(0,i.jsx)(e.li,{children:"What are the key optimization strategies for LLM integration?"}),"\n"]})]})}function g(n={}){const{wrapper:e}={...(0,o.R)(),...n.components};return e?(0,i.jsx)(e,{...n,children:(0,i.jsx)(p,{...n})}):p(n)}},8453:(n,e,t)=>{t.d(e,{R:()=>s});var a=t(6540);const i={},o=a.createContext(i);function s(n){const e=a.useContext(o);return a.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}}}]);