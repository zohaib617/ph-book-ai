"use strict";(globalThis.webpackChunkphysical_ai_humanoid_robotics=globalThis.webpackChunkphysical_ai_humanoid_robotics||[]).push([[978],{5954:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>l,contentTitle:()=>t,default:()=>m,frontMatter:()=>o,metadata:()=>s,toc:()=>d});const s=JSON.parse('{"id":"modules/module-2-digital-twin/sensor-simulation","title":"Environment & Sensor Simulation (LiDAR, Depth, IMU)","description":"Introduction","source":"@site/docs/modules/module-2-digital-twin/02-sensor-simulation.mdx","sourceDirName":"modules/module-2-digital-twin","slug":"/modules/module-2-digital-twin/sensor-simulation","permalink":"/docs/modules/module-2-digital-twin/sensor-simulation","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/modules/module-2-digital-twin/02-sensor-simulation.mdx","tags":[],"version":"current","sidebarPosition":2,"frontMatter":{"sidebar_label":"Environment & Sensor Simulation (LiDAR, Depth, IMU)","sidebar_position":2},"sidebar":"tutorialSidebar","previous":{"title":"Gazebo Physics: Gravity, Collisions","permalink":"/docs/modules/module-2-digital-twin/gazebo-physics"},"next":{"title":"Unity: High-Fidelity Rendering & Interaction","permalink":"/docs/modules/module-2-digital-twin/unity-rendering"}}');var a=i(4848),r=i(8453);const o={sidebar_label:"Environment & Sensor Simulation (LiDAR, Depth, IMU)",sidebar_position:2},t="Environment & Sensor Simulation (LiDAR, Depth, IMU)",l={},d=[{value:"Introduction",id:"introduction",level:2},{value:"Sensor Simulation in Gazebo",id:"sensor-simulation-in-gazebo",level:2},{value:"LiDAR Simulation",id:"lidar-simulation",level:2},{value:"Creating a LiDAR Sensor",id:"creating-a-lidar-sensor",level:3},{value:"LiDAR Parameters",id:"lidar-parameters",level:3},{value:"LiDAR Integration with Humanoid Robots",id:"lidar-integration-with-humanoid-robots",level:3},{value:"Depth Camera Simulation",id:"depth-camera-simulation",level:2},{value:"Creating a Depth Camera",id:"creating-a-depth-camera",level:3},{value:"Depth Camera Parameters",id:"depth-camera-parameters",level:3},{value:"IMU Simulation",id:"imu-simulation",level:2},{value:"Creating an IMU Sensor",id:"creating-an-imu-sensor",level:3},{value:"IMU Parameters",id:"imu-parameters",level:3},{value:"IMU Placement on Humanoid Robots",id:"imu-placement-on-humanoid-robots",level:3},{value:"Sensor Fusion in Simulation",id:"sensor-fusion-in-simulation",level:2},{value:"Creating a Multi-Sensor Setup",id:"creating-a-multi-sensor-setup",level:3},{value:"Noise Modeling",id:"noise-modeling",level:2},{value:"Noise Parameters",id:"noise-parameters",level:3},{value:"Environmental Effects on Sensors",id:"environmental-effects-on-sensors",level:2},{value:"Lighting Conditions",id:"lighting-conditions",level:3},{value:"Weather Simulation",id:"weather-simulation",level:3},{value:"Sensor Calibration in Simulation",id:"sensor-calibration-in-simulation",level:2},{value:"ROS Integration",id:"ros-integration",level:2},{value:"LiDAR Integration",id:"lidar-integration",level:3},{value:"Depth Camera Integration",id:"depth-camera-integration",level:3},{value:"IMU Integration",id:"imu-integration",level:3},{value:"Performance Optimization",id:"performance-optimization",level:2},{value:"Sensor Update Rates",id:"sensor-update-rates",level:3},{value:"Level of Detail",id:"level-of-detail",level:3},{value:"Best Practices for Sensor Simulation",id:"best-practices-for-sensor-simulation",level:2},{value:"Troubleshooting Common Issues",id:"troubleshooting-common-issues",level:2},{value:"Sensor Not Publishing Data",id:"sensor-not-publishing-data",level:3},{value:"Unrealistic Sensor Readings",id:"unrealistic-sensor-readings",level:3},{value:"Performance Issues",id:"performance-issues",level:3},{value:"RAG Summary",id:"rag-summary",level:2},{value:"Knowledge Check",id:"knowledge-check",level:2}];function c(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,r.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(n.header,{children:(0,a.jsx)(n.h1,{id:"environment--sensor-simulation-lidar-depth-imu",children:"Environment & Sensor Simulation (LiDAR, Depth, IMU)"})}),"\n",(0,a.jsx)(n.h2,{id:"introduction",children:"Introduction"}),"\n",(0,a.jsx)(n.p,{children:"Sensor simulation is a critical aspect of digital twin environments for humanoid robots. This chapter covers how to simulate various sensors including LiDAR, depth cameras, and IMUs in Gazebo, which are essential for perception and navigation in humanoid robotics."}),"\n",(0,a.jsx)(n.h2,{id:"sensor-simulation-in-gazebo",children:"Sensor Simulation in Gazebo"}),"\n",(0,a.jsx)(n.p,{children:"Gazebo provides realistic simulation of various sensor types by modeling their physical properties, noise characteristics, and environmental interactions. The simulation includes:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Physical modeling"}),": How sensors interact with the environment"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Noise modeling"}),": Realistic sensor noise and inaccuracies"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Dynamic effects"}),": Motion blur, latency, and other real-world effects"]}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"lidar-simulation",children:"LiDAR Simulation"}),"\n",(0,a.jsx)(n.p,{children:"LiDAR (Light Detection and Ranging) sensors are crucial for humanoid robots for navigation, mapping, and obstacle detection."}),"\n",(0,a.jsx)(n.h3,{id:"creating-a-lidar-sensor",children:"Creating a LiDAR Sensor"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-xml",children:'<sensor name="lidar_sensor" type="ray">\n  <pose>0.1 0 0.1 0 0 0</pose>\n  <ray>\n    <scan>\n      <horizontal>\n        <samples>720</samples>\n        <resolution>1</resolution>\n        <min_angle>-3.14159</min_angle>\n        <max_angle>3.14159</max_angle>\n      </horizontal>\n    </scan>\n    <range>\n      <min>0.1</min>\n      <max>30.0</max>\n      <resolution>0.01</resolution>\n    </range>\n  </ray>\n  <plugin name="lidar_controller" filename="libgazebo_ros_laser.so">\n    <topicName>/humanoid/laser_scan</topicName>\n    <frameName>lidar_link</frameName>\n    <min_intensity>0.1</min_intensity>\n  </plugin>\n</sensor>\n'})}),"\n",(0,a.jsx)(n.h3,{id:"lidar-parameters",children:"LiDAR Parameters"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Samples"}),": Number of rays in the horizontal scan"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Resolution"}),": Angular resolution of the sensor"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Range"}),": Minimum and maximum detection distance"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Field of View"}),": Angular coverage of the sensor"]}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"lidar-integration-with-humanoid-robots",children:"LiDAR Integration with Humanoid Robots"}),"\n",(0,a.jsx)(n.p,{children:"For humanoid robots, LiDAR placement is critical:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Head-mounted for 360\xb0 environment awareness"}),"\n",(0,a.jsx)(n.li,{children:"Chest-mounted for navigation and obstacle detection"}),"\n",(0,a.jsx)(n.li,{children:"Multiple sensors for redundancy and coverage"}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"depth-camera-simulation",children:"Depth Camera Simulation"}),"\n",(0,a.jsx)(n.p,{children:"Depth cameras provide 3D spatial information essential for humanoid robot perception."}),"\n",(0,a.jsx)(n.h3,{id:"creating-a-depth-camera",children:"Creating a Depth Camera"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-xml",children:'<sensor name="depth_camera" type="depth">\n  <camera>\n    <horizontal_fov>1.047</horizontal_fov>  \x3c!-- 60 degrees --\x3e\n    <image>\n      <width>640</width>\n      <height>480</height>\n      <format>R8G8B8</format>\n    </image>\n    <clip>\n      <near>0.1</near>\n      <far>10.0</far>\n    </clip>\n  </camera>\n  <plugin name="camera_controller" filename="libgazebo_ros_openni_kinect.so">\n    <baseline>0.2</baseline>\n    <alwaysOn>true</alwaysOn>\n    <updateRate>30.0</updateRate>\n    <cameraName>depth_camera</cameraName>\n    <imageTopicName>/humanoid/depth/image_raw</imageTopicName>\n    <depthImageTopicName>/humanoid/depth/image_depth</depthImageTopicName>\n    <pointCloudTopicName>/humanoid/depth/points</pointCloudTopicName>\n    <cameraInfoTopicName>/humanoid/depth/camera_info</cameraInfoTopicName>\n    <frameName>depth_camera_frame</frameName>\n    <pointCloudCutoff>0.1</pointCloudCutoff>\n    <distortion_k1>0.0</distortion_k1>\n    <distortion_k2>0.0</distortion_k2>\n    <distortion_k3>0.0</distortion_k3>\n    <distortion_t1>0.0</distortion_t1>\n    <distortion_t2>0.0</distortion_t2>\n    <CxPrime>0.0</CxPrime>\n    <Cx>0.0</Cx>\n    <Cy>0.0</Cy>\n    <focalLength>0.0</focalLength>\n    <hackBaseline>0.0</hackBaseline>\n  </plugin>\n</sensor>\n'})}),"\n",(0,a.jsx)(n.h3,{id:"depth-camera-parameters",children:"Depth Camera Parameters"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Field of View"}),": Angular coverage of the camera"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Resolution"}),": Image width and height in pixels"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Range"}),": Near and far clipping planes"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Update Rate"}),": Frequency of image capture"]}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"imu-simulation",children:"IMU Simulation"}),"\n",(0,a.jsx)(n.p,{children:"Inertial Measurement Units (IMUs) provide crucial orientation and acceleration data for humanoid robot balance and control."}),"\n",(0,a.jsx)(n.h3,{id:"creating-an-imu-sensor",children:"Creating an IMU Sensor"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-xml",children:'<sensor name="imu_sensor" type="imu">\n  <always_on>true</always_on>\n  <update_rate>100</update_rate>\n  <pose>0 0 0 0 0 0</pose>\n  <plugin name="imu_plugin" filename="libgazebo_ros_imu.so">\n    <topicName>/humanoid/imu</topicName>\n    <bodyName>imu_link</bodyName>\n    <serviceName>/humanoid/imu_service</serviceName>\n    <gaussianNoise>0.01</gaussianNoise>\n    <updateRateHZ>100.0</updateRateHZ>\n  </plugin>\n</sensor>\n'})}),"\n",(0,a.jsx)(n.h3,{id:"imu-parameters",children:"IMU Parameters"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Update Rate"}),": Frequency of IMU data updates"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Noise"}),": Gaussian noise parameters for realistic measurements"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Frame"}),": Coordinate frame for IMU measurements"]}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"imu-placement-on-humanoid-robots",children:"IMU Placement on Humanoid Robots"}),"\n",(0,a.jsx)(n.p,{children:"For humanoid robots, IMU placement affects balance control:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Torso for body orientation"}),"\n",(0,a.jsx)(n.li,{children:"Feet for ground contact detection"}),"\n",(0,a.jsx)(n.li,{children:"Head for visual stabilization"}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"sensor-fusion-in-simulation",children:"Sensor Fusion in Simulation"}),"\n",(0,a.jsx)(n.p,{children:"Combining multiple sensor inputs improves perception accuracy:"}),"\n",(0,a.jsx)(n.h3,{id:"creating-a-multi-sensor-setup",children:"Creating a Multi-Sensor Setup"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-xml",children:'<link name="sensor_mount">\n  \x3c!-- LiDAR --\x3e\n  <sensor name="lidar" type="ray">\n    \x3c!-- LiDAR configuration --\x3e\n  </sensor>\n\n  \x3c!-- IMU --\x3e\n  <sensor name="imu" type="imu">\n    \x3c!-- IMU configuration --\x3e\n  </sensor>\n\n  \x3c!-- Camera --\x3e\n  <sensor name="camera" type="camera">\n    \x3c!-- Camera configuration --\x3e\n  </sensor>\n</link>\n'})}),"\n",(0,a.jsx)(n.h2,{id:"noise-modeling",children:"Noise Modeling"}),"\n",(0,a.jsx)(n.p,{children:"Realistic noise modeling is essential for robust sensor algorithms:"}),"\n",(0,a.jsx)(n.h3,{id:"noise-parameters",children:"Noise Parameters"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-xml",children:'<sensor name="noisy_sensor" type="camera">\n  <camera>\n    \x3c!-- Camera parameters --\x3e\n  </camera>\n  <noise>\n    <type>gaussian</type>\n    <mean>0.0</mean>\n    <stddev>0.01</stddev>\n  </noise>\n</sensor>\n'})}),"\n",(0,a.jsx)(n.p,{children:"Types of noise to consider:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Gaussian noise"}),": Random measurement errors"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Bias"}),": Systematic measurement errors"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Drift"}),": Slow changes in sensor characteristics"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Quantization"}),": Discrete measurement effects"]}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"environmental-effects-on-sensors",children:"Environmental Effects on Sensors"}),"\n",(0,a.jsx)(n.h3,{id:"lighting-conditions",children:"Lighting Conditions"}),"\n",(0,a.jsx)(n.p,{children:"For camera sensors, lighting affects performance:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Bright sunlight causing saturation"}),"\n",(0,a.jsx)(n.li,{children:"Low light reducing visibility"}),"\n",(0,a.jsx)(n.li,{children:"Shadows affecting depth estimation"}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"weather-simulation",children:"Weather Simulation"}),"\n",(0,a.jsx)(n.p,{children:"Gazebo can simulate weather effects:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Rain reducing LiDAR range"}),"\n",(0,a.jsx)(n.li,{children:"Fog affecting camera visibility"}),"\n",(0,a.jsx)(n.li,{children:"Wind affecting IMU readings"}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"sensor-calibration-in-simulation",children:"Sensor Calibration in Simulation"}),"\n",(0,a.jsx)(n.p,{children:"Simulated sensors can have calibration parameters:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-xml",children:'<sensor name="calibrated_camera" type="camera">\n  <camera>\n    <distortion>\n      <k1>0.1</k1>\n      <k2>-0.2</k2>\n      <k3>0.1</k3>\n      <p1>0.01</p1>\n      <p2>0.01</p2>\n    </distortion>\n  </camera>\n</sensor>\n'})}),"\n",(0,a.jsx)(n.h2,{id:"ros-integration",children:"ROS Integration"}),"\n",(0,a.jsx)(n.p,{children:"Sensors in Gazebo integrate with ROS for data publishing:"}),"\n",(0,a.jsx)(n.h3,{id:"lidar-integration",children:"LiDAR Integration"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["Topic: ",(0,a.jsx)(n.code,{children:"/humanoid/laser_scan"})]}),"\n",(0,a.jsxs)(n.li,{children:["Message type: ",(0,a.jsx)(n.code,{children:"sensor_msgs/LaserScan"})]}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"depth-camera-integration",children:"Depth Camera Integration"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["Image topic: ",(0,a.jsx)(n.code,{children:"/humanoid/depth/image_raw"})]}),"\n",(0,a.jsxs)(n.li,{children:["Depth topic: ",(0,a.jsx)(n.code,{children:"/humanoid/depth/image_depth"})]}),"\n",(0,a.jsxs)(n.li,{children:["Point cloud: ",(0,a.jsx)(n.code,{children:"/humanoid/depth/points"})]}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"imu-integration",children:"IMU Integration"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["Topic: ",(0,a.jsx)(n.code,{children:"/humanoid/imu"})]}),"\n",(0,a.jsxs)(n.li,{children:["Message type: ",(0,a.jsx)(n.code,{children:"sensor_msgs/Imu"})]}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"performance-optimization",children:"Performance Optimization"}),"\n",(0,a.jsx)(n.h3,{id:"sensor-update-rates",children:"Sensor Update Rates"}),"\n",(0,a.jsx)(n.p,{children:"Balance accuracy with performance:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"High update rates: Better accuracy, higher computational cost"}),"\n",(0,a.jsx)(n.li,{children:"Low update rates: Lower computational cost, potential information loss"}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"level-of-detail",children:"Level of Detail"}),"\n",(0,a.jsx)(n.p,{children:"Adjust sensor complexity based on requirements:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"High detail: Accurate simulation, slower performance"}),"\n",(0,a.jsx)(n.li,{children:"Low detail: Faster simulation, reduced accuracy"}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"best-practices-for-sensor-simulation",children:"Best Practices for Sensor Simulation"}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Match Real Hardware"}),": Use parameters that match your real sensors"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Add Realistic Noise"}),": Include appropriate noise models"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Validate with Real Data"}),": Compare simulation to real sensor data"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Consider Computational Cost"}),": Balance accuracy with performance"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Test Edge Cases"}),": Validate behavior under challenging conditions"]}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"troubleshooting-common-issues",children:"Troubleshooting Common Issues"}),"\n",(0,a.jsx)(n.h3,{id:"sensor-not-publishing-data",children:"Sensor Not Publishing Data"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Check sensor configuration in URDF/SDF"}),"\n",(0,a.jsx)(n.li,{children:"Verify plugin loading"}),"\n",(0,a.jsx)(n.li,{children:"Check topic names and permissions"}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"unrealistic-sensor-readings",children:"Unrealistic Sensor Readings"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Validate noise parameters"}),"\n",(0,a.jsx)(n.li,{children:"Check coordinate frame alignment"}),"\n",(0,a.jsx)(n.li,{children:"Verify sensor placement"}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"performance-issues",children:"Performance Issues"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Reduce sensor update rates"}),"\n",(0,a.jsx)(n.li,{children:"Simplify sensor models"}),"\n",(0,a.jsx)(n.li,{children:"Optimize simulation step size"}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"rag-summary",children:"RAG Summary"}),"\n",(0,a.jsx)(n.p,{children:"Sensor simulation in Gazebo includes LiDAR for navigation and mapping, depth cameras for 3D perception, and IMUs for orientation and balance. Each sensor type has specific configuration parameters for realistic simulation, including noise modeling, environmental effects, and ROS integration. Proper sensor placement and calibration are crucial for humanoid robot applications."}),"\n",(0,a.jsx)(n.h2,{id:"knowledge-check",children:"Knowledge Check"}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsx)(n.li,{children:"What are the main sensor types simulated in Gazebo for humanoid robots?"}),"\n",(0,a.jsx)(n.li,{children:"How do you configure a LiDAR sensor in Gazebo?"}),"\n",(0,a.jsx)(n.li,{children:"What parameters affect depth camera simulation?"}),"\n",(0,a.jsx)(n.li,{children:"Why is IMU placement important for humanoid robots?"}),"\n",(0,a.jsx)(n.li,{children:"What are the key considerations for sensor noise modeling?"}),"\n"]})]})}function m(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(c,{...e})}):c(e)}},8453:(e,n,i)=>{i.d(n,{R:()=>o});var s=i(6540);const a={},r=s.createContext(a);function o(e){const n=s.useContext(r);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}}}]);