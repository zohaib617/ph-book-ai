{"allContent":{"docusaurus-plugin-content-docs":{"default":{"loadedVersions":[{"versionName":"current","label":"Next","banner":null,"badge":false,"noIndex":false,"className":"docs-version-current","path":"/docs","tagsPath":"/docs/tags","editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs","isLast":true,"routePriority":-1,"sidebarFilePath":"D:\\my-working-files\\ph_book\\ai-book2\\ai-book\\sidebars.js","contentPath":"D:\\my-working-files\\ph_book\\ai-book2\\ai-book\\docs","docs":[{"id":"intro/intro","title":"Introduction to Physical AI & Humanoid Robotics","description":"Welcome to the comprehensive educational book on Physical AI & Humanoid Robotics. This book covers the complete end-to-end robotics pipeline from middleware to perception to action, with a focus on modern approaches using ROS 2, simulation environments, AI perception systems, and Vision-Language-Action (VLA) systems.","source":"@site/docs/intro/intro.md","sourceDirName":"intro","slug":"/intro/","permalink":"/docs/intro/","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/intro/intro.md","tags":[],"version":"current","sidebarPosition":1,"frontMatter":{"sidebar_position":1},"sidebar":"tutorialSidebar","next":{"title":"Module 1: ROS 2 Fundamentals","permalink":"/docs/modules/module-1-ros2/"}},{"id":"modules/module-1-ros2/index","title":"Module 1: ROS 2 Fundamentals","description":"Welcome to Module 1 of the Physical AI & Humanoid Robotics book. This module covers the foundational concepts of ROS 2, which serves as the robotic nervous system for humanoid robots.","source":"@site/docs/modules/module-1-ros2/index.md","sourceDirName":"modules/module-1-ros2","slug":"/modules/module-1-ros2/","permalink":"/docs/modules/module-1-ros2/","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/modules/module-1-ros2/index.md","tags":[],"version":"current","sidebarPosition":1,"frontMatter":{"sidebar_position":1},"sidebar":"tutorialSidebar","previous":{"title":"Introduction to Physical AI & Humanoid Robotics","permalink":"/docs/intro/"},"next":{"title":"ROS 2 Nodes, Topics, Services","permalink":"/docs/modules/module-1-ros2/nodes-topics-services"}},{"id":"modules/module-1-ros2/nodes-topics-services","title":"ROS 2 Nodes, Topics, Services","description":"Introduction","source":"@site/docs/modules/module-1-ros2/01-nodes-topics-services.mdx","sourceDirName":"modules/module-1-ros2","slug":"/modules/module-1-ros2/nodes-topics-services","permalink":"/docs/modules/module-1-ros2/nodes-topics-services","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/modules/module-1-ros2/01-nodes-topics-services.mdx","tags":[],"version":"current","sidebarPosition":1,"frontMatter":{"sidebar_label":"ROS 2 Nodes, Topics, Services","sidebar_position":1},"sidebar":"tutorialSidebar","previous":{"title":"Module 1: ROS 2 Fundamentals","permalink":"/docs/modules/module-1-ros2/"},"next":{"title":"rclpy: Bridging Python Agents to ROS 2","permalink":"/docs/modules/module-1-ros2/rclpy-python-agent"}},{"id":"modules/module-1-ros2/rclpy-python-agent","title":"rclpy: Bridging Python Agents to ROS 2","description":"Introduction","source":"@site/docs/modules/module-1-ros2/02-rclpy-python-agent.mdx","sourceDirName":"modules/module-1-ros2","slug":"/modules/module-1-ros2/rclpy-python-agent","permalink":"/docs/modules/module-1-ros2/rclpy-python-agent","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/modules/module-1-ros2/02-rclpy-python-agent.mdx","tags":[],"version":"current","sidebarPosition":2,"frontMatter":{"sidebar_label":"rclpy: Bridging Python Agents to ROS 2","sidebar_position":2},"sidebar":"tutorialSidebar","previous":{"title":"ROS 2 Nodes, Topics, Services","permalink":"/docs/modules/module-1-ros2/nodes-topics-services"},"next":{"title":"URDF Basics for Humanoid Robots","permalink":"/docs/modules/module-1-ros2/urdf-humanoid"}},{"id":"modules/module-1-ros2/urdf-humanoid","title":"URDF Basics for Humanoid Robots","description":"Introduction","source":"@site/docs/modules/module-1-ros2/03-urdf-humanoid.mdx","sourceDirName":"modules/module-1-ros2","slug":"/modules/module-1-ros2/urdf-humanoid","permalink":"/docs/modules/module-1-ros2/urdf-humanoid","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/modules/module-1-ros2/03-urdf-humanoid.mdx","tags":[],"version":"current","sidebarPosition":3,"frontMatter":{"sidebar_label":"URDF Basics for Humanoid Robots","sidebar_position":3},"sidebar":"tutorialSidebar","previous":{"title":"rclpy: Bridging Python Agents to ROS 2","permalink":"/docs/modules/module-1-ros2/rclpy-python-agent"},"next":{"title":"Module 2: Digital Twin (Gazebo & Unity)","permalink":"/docs/modules/module-2-digital-twin/"}},{"id":"modules/module-2-digital-twin/gazebo-physics","title":"Gazebo Physics: Gravity, Collisions","description":"Introduction","source":"@site/docs/modules/module-2-digital-twin/01-gazebo-physics.mdx","sourceDirName":"modules/module-2-digital-twin","slug":"/modules/module-2-digital-twin/gazebo-physics","permalink":"/docs/modules/module-2-digital-twin/gazebo-physics","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/modules/module-2-digital-twin/01-gazebo-physics.mdx","tags":[],"version":"current","sidebarPosition":1,"frontMatter":{"sidebar_label":"Gazebo Physics: Gravity, Collisions","sidebar_position":1},"sidebar":"tutorialSidebar","previous":{"title":"Module 2: Digital Twin (Gazebo & Unity)","permalink":"/docs/modules/module-2-digital-twin/"},"next":{"title":"Environment & Sensor Simulation (LiDAR, Depth, IMU)","permalink":"/docs/modules/module-2-digital-twin/sensor-simulation"}},{"id":"modules/module-2-digital-twin/index","title":"Module 2: Digital Twin (Gazebo & Unity)","description":"Welcome to Module 2 of the Physical AI & Humanoid Robotics book. This module explores digital twin concepts using simulation environments, focusing on physics simulation, sensor modeling, and high-fidelity rendering.","source":"@site/docs/modules/module-2-digital-twin/index.md","sourceDirName":"modules/module-2-digital-twin","slug":"/modules/module-2-digital-twin/","permalink":"/docs/modules/module-2-digital-twin/","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/modules/module-2-digital-twin/index.md","tags":[],"version":"current","sidebarPosition":2,"frontMatter":{"sidebar_position":2},"sidebar":"tutorialSidebar","previous":{"title":"URDF Basics for Humanoid Robots","permalink":"/docs/modules/module-1-ros2/urdf-humanoid"},"next":{"title":"Gazebo Physics: Gravity, Collisions","permalink":"/docs/modules/module-2-digital-twin/gazebo-physics"}},{"id":"modules/module-2-digital-twin/sensor-simulation","title":"Environment & Sensor Simulation (LiDAR, Depth, IMU)","description":"Introduction","source":"@site/docs/modules/module-2-digital-twin/02-sensor-simulation.mdx","sourceDirName":"modules/module-2-digital-twin","slug":"/modules/module-2-digital-twin/sensor-simulation","permalink":"/docs/modules/module-2-digital-twin/sensor-simulation","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/modules/module-2-digital-twin/02-sensor-simulation.mdx","tags":[],"version":"current","sidebarPosition":2,"frontMatter":{"sidebar_label":"Environment & Sensor Simulation (LiDAR, Depth, IMU)","sidebar_position":2},"sidebar":"tutorialSidebar","previous":{"title":"Gazebo Physics: Gravity, Collisions","permalink":"/docs/modules/module-2-digital-twin/gazebo-physics"},"next":{"title":"Unity: High-Fidelity Rendering & Interaction","permalink":"/docs/modules/module-2-digital-twin/unity-rendering"}},{"id":"modules/module-2-digital-twin/unity-rendering","title":"Unity: High-Fidelity Rendering & Interaction","description":"Introduction","source":"@site/docs/modules/module-2-digital-twin/03-unity-rendering.mdx","sourceDirName":"modules/module-2-digital-twin","slug":"/modules/module-2-digital-twin/unity-rendering","permalink":"/docs/modules/module-2-digital-twin/unity-rendering","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/modules/module-2-digital-twin/03-unity-rendering.mdx","tags":[],"version":"current","sidebarPosition":3,"frontMatter":{"sidebar_label":"Unity: High-Fidelity Rendering & Interaction","sidebar_position":3},"sidebar":"tutorialSidebar","previous":{"title":"Environment & Sensor Simulation (LiDAR, Depth, IMU)","permalink":"/docs/modules/module-2-digital-twin/sensor-simulation"},"next":{"title":"Module 3: AI-Robot Brain (NVIDIA Isaac™)","permalink":"/docs/modules/module-3-ai-brain/"}},{"id":"modules/module-3-ai-brain/index","title":"Module 3: AI-Robot Brain (NVIDIA Isaac™)","description":"Welcome to Module 3 of the Physical AI & Humanoid Robotics book. This module focuses on AI-powered perception and navigation systems using NVIDIA Isaac technologies.","source":"@site/docs/modules/module-3-ai-brain/index.md","sourceDirName":"modules/module-3-ai-brain","slug":"/modules/module-3-ai-brain/","permalink":"/docs/modules/module-3-ai-brain/","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/modules/module-3-ai-brain/index.md","tags":[],"version":"current","sidebarPosition":3,"frontMatter":{"sidebar_position":3},"sidebar":"tutorialSidebar","previous":{"title":"Unity: High-Fidelity Rendering & Interaction","permalink":"/docs/modules/module-2-digital-twin/unity-rendering"},"next":{"title":"Isaac Sim: Photorealistic Simulation + Synthetic Data","permalink":"/docs/modules/module-3-ai-brain/isaac-sim"}},{"id":"modules/module-3-ai-brain/isaac-ros-pipelines","title":"Isaac ROS Pipelines: VSLAM, Navigation","description":"Introduction","source":"@site/docs/modules/module-3-ai-brain/02-isaac-ros-pipelines.mdx","sourceDirName":"modules/module-3-ai-brain","slug":"/modules/module-3-ai-brain/isaac-ros-pipelines","permalink":"/docs/modules/module-3-ai-brain/isaac-ros-pipelines","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/modules/module-3-ai-brain/02-isaac-ros-pipelines.mdx","tags":[],"version":"current","sidebarPosition":2,"frontMatter":{"sidebar_label":"Isaac ROS Pipelines: VSLAM, Navigation","sidebar_position":2},"sidebar":"tutorialSidebar","previous":{"title":"Isaac Sim: Photorealistic Simulation + Synthetic Data","permalink":"/docs/modules/module-3-ai-brain/isaac-sim"},"next":{"title":"Nav2 for Bipedal Humanoid Movement","permalink":"/docs/modules/module-3-ai-brain/nav2-bipedal"}},{"id":"modules/module-3-ai-brain/isaac-sim","title":"Isaac Sim: Photorealistic Simulation + Synthetic Data","description":"Introduction","source":"@site/docs/modules/module-3-ai-brain/01-isaac-sim.mdx","sourceDirName":"modules/module-3-ai-brain","slug":"/modules/module-3-ai-brain/isaac-sim","permalink":"/docs/modules/module-3-ai-brain/isaac-sim","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/modules/module-3-ai-brain/01-isaac-sim.mdx","tags":[],"version":"current","sidebarPosition":1,"frontMatter":{"sidebar_label":"Isaac Sim: Photorealistic Simulation + Synthetic Data","sidebar_position":1},"sidebar":"tutorialSidebar","previous":{"title":"Module 3: AI-Robot Brain (NVIDIA Isaac™)","permalink":"/docs/modules/module-3-ai-brain/"},"next":{"title":"Isaac ROS Pipelines: VSLAM, Navigation","permalink":"/docs/modules/module-3-ai-brain/isaac-ros-pipelines"}},{"id":"modules/module-3-ai-brain/nav2-bipedal","title":"Nav2 for Bipedal Humanoid Movement","description":"Introduction","source":"@site/docs/modules/module-3-ai-brain/03-nav2-bipedal.mdx","sourceDirName":"modules/module-3-ai-brain","slug":"/modules/module-3-ai-brain/nav2-bipedal","permalink":"/docs/modules/module-3-ai-brain/nav2-bipedal","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/modules/module-3-ai-brain/03-nav2-bipedal.mdx","tags":[],"version":"current","sidebarPosition":3,"frontMatter":{"sidebar_label":"Nav2 for Bipedal Humanoid Movement","sidebar_position":3},"sidebar":"tutorialSidebar","previous":{"title":"Isaac ROS Pipelines: VSLAM, Navigation","permalink":"/docs/modules/module-3-ai-brain/isaac-ros-pipelines"},"next":{"title":"Module 4: Vision-Language-Action (VLA)","permalink":"/docs/modules/module-4-vla/"}},{"id":"modules/module-4-vla/autonomous-humanoid","title":"Capstone: Autonomous Humanoid Pipeline","description":"Introduction","source":"@site/docs/modules/module-4-vla/03-autonomous-humanoid.mdx","sourceDirName":"modules/module-4-vla","slug":"/modules/module-4-vla/autonomous-humanoid","permalink":"/docs/modules/module-4-vla/autonomous-humanoid","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/modules/module-4-vla/03-autonomous-humanoid.mdx","tags":[],"version":"current","sidebarPosition":3,"frontMatter":{"sidebar_label":"Capstone: Autonomous Humanoid Pipeline","sidebar_position":3},"sidebar":"tutorialSidebar","previous":{"title":"Cognitive Planning (LLMs → ROS 2 actions)","permalink":"/docs/modules/module-4-vla/llm-cognitive-planning"}},{"id":"modules/module-4-vla/index","title":"Module 4: Vision-Language-Action (VLA)","description":"Welcome to Module 4 of the Physical AI & Humanoid Robotics book. This module focuses on Vision-Language-Action systems that enable humanoid robots to understand and respond to human commands through voice and visual input.","source":"@site/docs/modules/module-4-vla/index.md","sourceDirName":"modules/module-4-vla","slug":"/modules/module-4-vla/","permalink":"/docs/modules/module-4-vla/","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/modules/module-4-vla/index.md","tags":[],"version":"current","sidebarPosition":4,"frontMatter":{"sidebar_position":4},"sidebar":"tutorialSidebar","previous":{"title":"Nav2 for Bipedal Humanoid Movement","permalink":"/docs/modules/module-3-ai-brain/nav2-bipedal"},"next":{"title":"Voice-to-Action (Whisper)","permalink":"/docs/modules/module-4-vla/whisper-voice-action"}},{"id":"modules/module-4-vla/llm-cognitive-planning","title":"Cognitive Planning (LLMs → ROS 2 actions)","description":"Introduction","source":"@site/docs/modules/module-4-vla/02-llm-cognitive-planning.mdx","sourceDirName":"modules/module-4-vla","slug":"/modules/module-4-vla/llm-cognitive-planning","permalink":"/docs/modules/module-4-vla/llm-cognitive-planning","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/modules/module-4-vla/02-llm-cognitive-planning.mdx","tags":[],"version":"current","sidebarPosition":2,"frontMatter":{"sidebar_label":"Cognitive Planning (LLMs → ROS 2 actions)","sidebar_position":2},"sidebar":"tutorialSidebar","previous":{"title":"Voice-to-Action (Whisper)","permalink":"/docs/modules/module-4-vla/whisper-voice-action"},"next":{"title":"Capstone: Autonomous Humanoid Pipeline","permalink":"/docs/modules/module-4-vla/autonomous-humanoid"}},{"id":"modules/module-4-vla/whisper-voice-action","title":"Voice-to-Action (Whisper)","description":"Introduction","source":"@site/docs/modules/module-4-vla/01-whisper-voice-action.mdx","sourceDirName":"modules/module-4-vla","slug":"/modules/module-4-vla/whisper-voice-action","permalink":"/docs/modules/module-4-vla/whisper-voice-action","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/modules/module-4-vla/01-whisper-voice-action.mdx","tags":[],"version":"current","sidebarPosition":1,"frontMatter":{"sidebar_label":"Voice-to-Action (Whisper)","sidebar_position":1},"sidebar":"tutorialSidebar","previous":{"title":"Module 4: Vision-Language-Action (VLA)","permalink":"/docs/modules/module-4-vla/"},"next":{"title":"Cognitive Planning (LLMs → ROS 2 actions)","permalink":"/docs/modules/module-4-vla/llm-cognitive-planning"}}],"drafts":[],"sidebars":{"tutorialSidebar":[{"type":"doc","id":"intro/intro","label":"Introduction to Physical AI & Humanoid Robotics"},{"type":"category","label":"modules","collapsible":true,"collapsed":true,"items":[{"type":"category","label":"Module 1: ROS 2 Fundamentals","collapsible":true,"collapsed":true,"items":[{"type":"doc","id":"modules/module-1-ros2/nodes-topics-services","label":"ROS 2 Nodes, Topics, Services"},{"type":"doc","id":"modules/module-1-ros2/rclpy-python-agent","label":"rclpy: Bridging Python Agents to ROS 2"},{"type":"doc","id":"modules/module-1-ros2/urdf-humanoid","label":"URDF Basics for Humanoid Robots"}],"link":{"type":"doc","id":"modules/module-1-ros2/index"}},{"type":"category","label":"Module 2: Digital Twin (Gazebo & Unity)","collapsible":true,"collapsed":true,"items":[{"type":"doc","id":"modules/module-2-digital-twin/gazebo-physics","label":"Gazebo Physics: Gravity, Collisions"},{"type":"doc","id":"modules/module-2-digital-twin/sensor-simulation","label":"Environment & Sensor Simulation (LiDAR, Depth, IMU)"},{"type":"doc","id":"modules/module-2-digital-twin/unity-rendering","label":"Unity: High-Fidelity Rendering & Interaction"}],"link":{"type":"doc","id":"modules/module-2-digital-twin/index"}},{"type":"category","label":"Module 3: AI-Robot Brain (NVIDIA Isaac™)","collapsible":true,"collapsed":true,"items":[{"type":"doc","id":"modules/module-3-ai-brain/isaac-sim","label":"Isaac Sim: Photorealistic Simulation + Synthetic Data"},{"type":"doc","id":"modules/module-3-ai-brain/isaac-ros-pipelines","label":"Isaac ROS Pipelines: VSLAM, Navigation"},{"type":"doc","id":"modules/module-3-ai-brain/nav2-bipedal","label":"Nav2 for Bipedal Humanoid Movement"}],"link":{"type":"doc","id":"modules/module-3-ai-brain/index"}},{"type":"category","label":"Module 4: Vision-Language-Action (VLA)","collapsible":true,"collapsed":true,"items":[{"type":"doc","id":"modules/module-4-vla/whisper-voice-action","label":"Voice-to-Action (Whisper)"},{"type":"doc","id":"modules/module-4-vla/llm-cognitive-planning","label":"Cognitive Planning (LLMs → ROS 2 actions)"},{"type":"doc","id":"modules/module-4-vla/autonomous-humanoid","label":"Capstone: Autonomous Humanoid Pipeline"}],"link":{"type":"doc","id":"modules/module-4-vla/index"}}]}]}}]}},"docusaurus-plugin-content-blog":{"default":{"blogSidebarTitle":"Recent posts","blogPosts":[],"blogListPaginated":[],"blogTags":{},"blogTagsListPath":"/blog/tags"}},"docusaurus-plugin-content-pages":{"default":[{"type":"jsx","permalink":"/","source":"@site/src/pages/index.js"}]},"docusaurus-plugin-debug":{},"docusaurus-plugin-svgr":{},"docusaurus-theme-classic":{},"docusaurus-bootstrap-plugin":{},"docusaurus-mdx-fallback-plugin":{}}}